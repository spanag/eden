{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: A network of detailed cells\n",
    "<span id=\"top\" />\n",
    "This tutorial shows how to build an active network of detailed neurons, simulate it to get the neurons' potentials over time in full spatial detail, and display these data as an animated 3D display.\n",
    "<!-- , or a rendered movie file LATER. -->\n",
    "\n",
    "Refer also to the previous introductory articles for [the basics of NeuroML]( intro_neuroml.ipynb ) and [spatially detailed cells]( intro_spatial.ipynb )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "First of all, let's install the required software, if on certain platforms like Colab that run the bare notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; from pathlib import Path\n",
    "if 'COLAB_BACKEND_VERSION' in os.environ:\n",
    "  !TMP=$(mktemp -d); git clone https://eden-simulator.org/repo --depth 1 -b development \"$TMP\"; cp -r \"$TMP/.\" .; rm -rf \"$TMP\"\n",
    "  exec(Path('.binder/install_livenb.py').read_text())\n",
    "if 'DEEPNOTE_PROJECT_ID' in os.environ: exec(Path('../.binder/install_livenb.py').read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the network\n",
    "\n",
    "Building a network model generally incolves the following actions:\n",
    "\n",
    "- place the *cells* forming the network;\n",
    "- connect the cells with *synapses*;\n",
    "- add the experimental *rig*: add external stimuli and probe the electro-chemical variables of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing neurons\n",
    "First, let's decide where to place the neurons in the model.  For this tutorial, assume that neurons are uniformly spread over, say a tall cylinder that stands for a microcolumn.  We'll use an evenly spread [*quasi*-pseudo-random distribution](https://en.wikipedia.org/wiki/Low-discrepancy_sequence), which won't randomly form misleading clumps like a true random sample would. We'll also sort the cells by distance along the cylinder, to give more  meaning to the the resulting rasters and correlation matrices.\n",
    "\n",
    "Check the following code also for a neat way to get uniformly random points over an arbitrary shape:\n",
    "\n",
    "- check they fall on the shape, keep only those who do.\n",
    "- resample to fill in the remaining slots.\n",
    "\n",
    "Kind of like painting with a stencil :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q 'scipy>=1.12' # for an evenly spread, *quasi* pseudo random distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a few cells\n",
    "# randomize x, y, z using e.g. a cylinder\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.Halton.html#scipy.stats.qmc.Halton\n",
    "seed = 123\n",
    "N_cells = 30\n",
    "# Points will be placed in a cylinder of given radius and height\n",
    "region_radius = 50 # microns\n",
    "region_height = 200\n",
    "\n",
    "from scipy.stats import qmc\n",
    "position_rng = qmc.Halton(d=3, scramble=True, seed=seed)\n",
    "\n",
    "# Use rejection, to make the 3D points conform to an arbitrary domain\n",
    "remaining_cells = N_cells\n",
    "cell_positions = []\n",
    "while len(cell_positions) < N_cells:\n",
    "    # Get points over a uniform box\n",
    "    point_samples = (\n",
    "        position_rng.random(n=N_cells-len(cell_positions)) \n",
    "        * np.array([2*region_radius,region_height,2*region_radius]) \n",
    "        - np.array([region_radius,0,region_radius]) #list()\n",
    "    )\n",
    "    valid_points = [ (x,y,z) for (x,y,z) in point_samples if (x**2+z**2<region_radius**2) and (y > 0 and y < region_height) ]\n",
    "    cell_positions += valid_points\n",
    "cell_positions = np.array(cell_positions)\n",
    "\n",
    "# Also! Sort by height to make our lives easier!\n",
    "cell_positions = cell_positions[np.argsort(cell_positions[:,1])]\n",
    "# cell_positions is now ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the distribution visually. If running the notebook, rotate the display to see the distribution from all sides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure(1); fig.clear(); fig.set_size_inches(4,5)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "# Note: usually in SWC and NeuroML files, +Y is 'up', zero is the soma middle, and units are in microns.\n",
    "# But these also vary with the provenance of the files, always check before using.\n",
    "# Swap Y and Z for viz purposes.\n",
    "for i, (x, z, y) in enumerate(cell_positions):\n",
    "    ax.text(x, y, z, f'{i}', color='green')\n",
    "# Tweaking display region and labels\n",
    "ax.set_xlim(-region_radius,+region_radius)\n",
    "ax.set_ylim(-region_radius,+region_radius)\n",
    "ax.set_zlim(0, region_height)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('Width (Î¼m)'); ax.set_ylabel('Depth (Î¼m)'); ax.set_zlabel('Height (Î¼m)')\n",
    "ax.view_init(elev=9, azim=-50, roll=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding synapses\n",
    "Now let's hook up the neurons with synapses that will cause the cells to [stimulate]( https://en.wikipedia.org/wiki/Excitatory_postsynaptic_potential ) each other, creating an interesting effect. To keep the code simple, we'll consider connections over each pair of cells (it's fine if there are not thousands of them).\n",
    "\n",
    "We will assume *distance-dependent* _probability_ (of each pair-wise synapse forming at all), _weight_ (slightly randomized) and _delay_ (linear with distance). Check the code for the specific formulae.\n",
    "\n",
    "For this tutorial, we'll only consider soma-to-soma connections (i.e. between `<segment>`s `0` of each); feel free to use your preferred models and methods.  Refer to the [relevant chapter]( intro_spatial.ipynb ) on how to handle spatially-detailed neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "synapses_seed = 123\n",
    "rng = default_rng(synapses_seed)\n",
    "\n",
    "# The range constant (sigma, in this case) for the synapses.\n",
    "syn_radius = 100\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "d_mat = squareform(pdist(cell_positions)) # Distance between all pairs of cell somata\n",
    "p_mat = 0.7*np.exp(-(d_mat/syn_radius)**2) # Chance for connection = f(distance)\n",
    "p_mat = p_mat - np.diag(np.diag(p_mat)) # Remove the diagonal\n",
    "\n",
    "w_mat = 0.5*(rng.standard_normal(d_mat.shape)+2) * np.exp(-(d_mat/syn_radius)**2) # Variable weight between pairs\n",
    "t_mat = 0.001*np.exp(-(d_mat/syn_radius)**2) # Deterministic delay between pairs, in seconds\n",
    "\n",
    "pre = []; post = []; weight = []; delay = []; # Parallel pre/post synaptic cell, weight, delay for ...\n",
    "for pre_cell in range(N_cells):\n",
    "    post_cells = (np.argwhere(p_mat[:,pre_cell] > rng.random(N_cells))).flatten() # Get synapse targets that pass the Bernoulli test\n",
    "#     print(post_cells)\n",
    "    pre += ([pre_cell] * len(post_cells));  # append the relevant syn pairs, with weight and delay\n",
    "    post.extend(post_cells);\n",
    "    weight.extend(w_mat[pre_cell,post_cells]);\n",
    "    delay.extend(t_mat[pre_cell,post_cells]);\n",
    "# print( \"from:\", pre, \"\\nto  :\", post, \"\\nwei :\", weight, \"\\ndel :\", delay )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the per-neuron-pair matrices of the factors involved.\n",
    "\n",
    "First, tabulate the absolute distance, and the probability of a connection for each neuron pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "axs = plt.figure(figsize=(11,4)).subplots(nrows=1, ncols=2);\n",
    "axs[0].set_title('Distance (Î¼m)'); im = axs[0].imshow(d_mat, cmap='Reds'); fig.colorbar(im, ax=axs[0]);\n",
    "axs[1].set_title('P(Syn)'); im = axs[1].imshow(p_mat, cmap='Blues'); fig.colorbar(im, ax=axs[1]);\n",
    "for i in range(axs.shape[0]): axs[i].set_ylabel('From neuron'); axs[i].set_xlabel('To neuron');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, tabulate weight and delay for the connections that were actually realized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_2d = np.zeros((N_cells,N_cells))*np.nan; d_2d = w_2d + 0 # full mask\n",
    "w_2d[pre,post] = weight; d_2d[pre,post] = delay # fill in the values that apply\n",
    "\n",
    "axs = plt.figure(figsize=(11,4)).subplots(nrows=1, ncols=2);\n",
    "axs[0].set_title('Weight'); im = axs[0].imshow(w_2d, cmap='YlOrBr'); fig.colorbar(im, ax=axs[0]); # could also use a diverging cmap centered at 0\n",
    "axs[1].set_title('Delay (msec)'); im = axs[1].imshow(d_2d*1000, cmap='Greens'); fig.colorbar(im, ax=axs[1]);\n",
    "for i in range(axs.shape[0]): axs[i].set_ylabel('From neuron'); axs[i].set_xlabel('To neuron');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding stimuli\n",
    "\n",
    "Now let's play with the network through unnatural means -- for example, apply a one-off DC clamp to cells in the bottom 20% of the cylinder.\n",
    "\n",
    "The attentive reader may have noticed that what's been specified so far is not specific to NeuroML.  That's because NeuroML can run any *distribution* of cells and synapses just the same; they can be constructed independently with any method, and expressed in the NeuroML data format just before runnimg the model.\n",
    "\n",
    "The more involved and detailed descriptions(neuron shape, chemistry, dynamics and such) for the individual parts of the model will be specified in NeuroML, in the section right after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_cells = [i for i, (x,y,z) in enumerate(cell_positions) if y < region_height/5]\n",
    "stim_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating NeuroML for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to make the NeuroML files in order to run the simulation.\n",
    "Let's put them in a sub-folder of the working directory, to avoid polluting the folder that the notebook is in too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nml_dir = 'tut_net/'\n",
    "os.makedirs(nml_dir, exist_ok=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cell's description, let's cheat and grab an existing description from the NeuroML-DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmldb_cell_name = 'NMLCL000625'\n",
    "zip_file_name = f'{nmldb_cell_name}.zip'\n",
    "# Download the zip file with the model\n",
    "import urllib.request\n",
    "# Because NMLDB is having some issues with HTTPS, don't demand HTTPS verification\n",
    "import ssl; ssl._create_default_https_context = ssl._create_unverified_context\n",
    "urllib.request.urlretrieve(f'http://neuroml-db.org/GetModelZip?modelID={nmldb_cell_name}&version=NeuroML', zip_file_name)\n",
    "# and unpack it\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(zip_file_name, 'r') as zipp: zipp.extractall(nml_dir+nmldb_cell_name+'/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need the cell type's identifier from the NML file, which for some reason is not the `NMLCLxxxxxx` identifier. (Neither is the NML file)  \n",
    "This is the same as the `<name>.cell.nml` filename, so let's scan for that.  \n",
    "Also, the `<spikeThresh>` to register an action potential is not specified for some reason.  Because we're using classical chemical synapses, we'll have to add it to `<biophysicalProperties>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATER get it with libNML or with the NMLDB API.\n",
    "import os\n",
    "celltype_name = None\n",
    "cell_nml_file_suffix = '.cell.nml'\n",
    "for filename in os.listdir(nml_dir+nmldb_cell_name):\n",
    "    # print(filename)\n",
    "    if not filename.endswith(cell_nml_file_suffix): continue # get the cell.nml files\n",
    "    new_celltype_name = filename[:-len(cell_nml_file_suffix)] # there it is\n",
    "    if celltype_name: raise ValueError(f'cell type: is it {new_celltype_name} or {new_celltype_name}') # :c\n",
    "    celltype_name = new_celltype_name\n",
    "print('Celltype name:', celltype_name)\n",
    "\n",
    "# Also modify the NML file to add spikeThresh\n",
    "cell_filename = nml_dir+nmldb_cell_name+'/'+new_celltype_name+cell_nml_file_suffix\n",
    "# Read in the file\n",
    "with open(cell_filename, 'r') as file: filedata = file.read()\n",
    "# Replace the target string\n",
    "if '<spikeThresh' not in filedata:filedata = filedata.replace('</membraneProperties>','<spikeThresh value=\"0 mV\"/></membraneProperties>')\n",
    "# Write the file out again\n",
    "with open(cell_filename, 'w') as file: file.write(filedata)\n",
    "# print(filedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some routines to construct a NeuroML file incrementally from the population and projection data we collected so far, into one big string.\n",
    "There are also other ways to create and manipulate NeuroML files,  but this one here makes for a more direct demonstration.\n",
    "<!-- LATER NeuroML refrence and external resources -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NmlHeader():\n",
    "    return '''<neuroml xmlns=\"http://www.neuroml.org/schema/neuroml2\"  xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.neuroml.org/schema/neuroml2 https://raw.github.com/NeuroML/NeuroML2/development/Schemas/NeuroML2/NeuroML_v2.1.xsd\">\n",
    "    <include file=\"sim_components.nml\"/>'''\n",
    "def NmlNetworkHeader(): return '''\\n   <network id=\"Net\" type=\"networkWithTemperature\" temperature=\"37degC\">'''\n",
    "def NmlNetworkFooter(): return '''\\n    </network>'''\n",
    "\n",
    "def NmlPopulation(pop_name, celltype_name, cell_positions):\n",
    "    return f'''\n",
    "        <population id=\"{pop_name}\" component=\"{celltype_name}\" size=\"{len(cell_positions)}\">\n",
    "        '''+'\\n\\t'.join([\n",
    "            f'  <instance id=\"{i}\"><location x=\"{x}\" y=\"{y}\" z=\"{z}\"/></instance>'\n",
    "            for i, (x,y,z) in enumerate(cell_positions) ])+'''\n",
    "        </population>\n",
    "    '''\n",
    "\n",
    "def NmlSynapticProjection(\n",
    "    proj_name,syncomp_name, pre_pop_name, post_pop_name,\n",
    "    preCell, postCell, weight=None, delay=None, preSeg=None, postSeg = None, preFrac = None, postFrac = None):\n",
    "    if delay is None: delay = [0]*len(preCell)\n",
    "    if weight is None: weight = [1]*len(preCell)\n",
    "    if preSeg is None: preSeg = [0]*len(preCell)\n",
    "    if postSeg is None: postSeg = [0]*len(preCell)\n",
    "    if preFrac is None: preFrac = [0]*len(preCell)\n",
    "    if postFrac is None: postFrac = [0]*len(preCell)\n",
    "    \n",
    "    return f'''\n",
    "        <projection id=\"{proj_name}\" synapse=\"{syncomp_name}\" presynapticPopulation=\"{pre_pop_name}\" postsynapticPopulation=\"{post_pop_name}\">\n",
    "            '''+'\\n\\t    '.join([\n",
    "            f'<connectionWD id=\"{i}\" preCellId=\"{preCell[i]}\" postCellId=\"{postCell[i]}\" '+\n",
    "            f'weight=\"{weight[i]}\" delay=\"{delay[i]} s\" '+\n",
    "            f'preSegmentId=\"{preSeg[i]}\" postSegmentId=\"{postSeg[i]}\" '+\n",
    "            f'preFractionAlong=\"{preFrac[i]}\" postFractionAlong=\"{postFrac[i]}\"/>'\n",
    "            for i in range(len(pre)) ])+'''\n",
    "        </projection>\n",
    "    '''\n",
    "    \n",
    "def NmlInputList(input_list_name,stim_component,target_pop_name,cells, segs=None, fracs=None):\n",
    "    if segs is None: segs = [0]*len(cells)\n",
    "    if fracs is None: fracs = [0]*len(cells)\n",
    "    return f'''\n",
    "        <inputList id=\"{input_list_name}\" population=\"{target_pop_name}\" component=\"{stim_component}\">\n",
    "        '''+'\\n\\t'.join([\n",
    "            f'  <input id=\"{i}\" target=\"../{target_pop_name}[{cells[i]}]\" '+\n",
    "            f'segmentId=\"{segs[i]}\" fractionAlong=\"{fracs[i]}\" destination=\"synapses\"/>'\n",
    "            for i in range(len(cells)) ])+'''\n",
    "        </inputList>\n",
    "    '''\n",
    "\n",
    "def NmlFooter():return '''\\n</neuroml>'''\n",
    "\n",
    "# celltype_name = 'cACint209_L6_NBC_a3972c5d97_0_0'\n",
    "# celltype_name = 'cADpyr232_L5_TTPC2_8bab918b58_0_0'\n",
    "# celltype_name = 'dNAC222_L6_SBC_194972ee43_0_0'\n",
    "\n",
    "population_name = 'MyCells'\n",
    "\n",
    "# Write the network file.\n",
    "with open(nml_dir+\"/example.nml\", \"w\") as f:\n",
    "    f.write(NmlHeader()); f.write(NmlNetworkHeader())\n",
    "    f.write(NmlPopulation(population_name, celltype_name, cell_positions))\n",
    "    f.write(NmlSynapticProjection('FirstSynProjection', 'NMDA', population_name, population_name, pre, post, weight, delay))\n",
    "    f.write(NmlInputList('FirstStimList', 'MyStim', population_name, stim_cells))\n",
    "    f.write(NmlNetworkFooter())\n",
    "    f.write(NmlFooter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After writing the `net.nml` representing the network made up of cells, synapses and input stimuli, we'll make a companion file \n",
    "`LEMS_<something>.xml` (this is the convention; I guess you could also use `.sim.xml` to make the name further clearer.)\n",
    "\n",
    "Here, we'll specify some parameters to run the simulation like for how long and with how big a timestep, but also add the last part of the rig: recording certain trajectories and spike trains from the simulated model. Since neurites in each cell usually fire togher(in sequence), we'll record fthe membrane voltage trajectories for the soma of each neuron.  (Conveniently enough, if the location on the neuron is not specified in a NeuroML path, the location of the soma is used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sim file.\n",
    "def NmlSimParms(pop_name, N_cells, run_time, run_timestep=25e-6):\n",
    "    return f'''\n",
    "        <Simulation id=\"sim1\" length=\"{run_time}s\" step=\"{run_timestep}s\" target=\"Net\">\n",
    "            <OutputFile id=\"first\" fileName=\"tut_net/results.gen.txt\">\n",
    "            '''+'\\n\\t        '.join([f'<OutputColumn id=\"v_{i}\" quantity=\"{pop_name}[{i}]/v\"/>' for i in range(N_cells)])+'''\n",
    "            </OutputFile>\n",
    "        </Simulation>\n",
    "        <Target component=\"sim1\"/>'''\n",
    "\n",
    "with open(nml_dir+\"/Sim.xml\", \"w\") as f:   \n",
    "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Lems>\\n<include file=\"example.nml\"/>\\n')\n",
    "    f.write(NmlSimParms(population_name, N_cells, run_time=0.25, run_timestep=25e-6))\n",
    "#     f.write(NmlSimParmss(population_name,celltype_name,None, run_time=0.25, run_timestep=25e-6))\n",
    "    f.write('\\n</Lems>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $nml_dir/sim_components.nml\n",
    "<neuroml>\n",
    "    <include file=\"NMLCL000625/cACint209_L6_NBC_a3972c5d97_0_0.cell.nml\"/>\n",
    "    <expTwoSynapse id=\"NMDA\" gbase=\".5nS\" erev=\"0mV\" tauDecay=\"15ms\" tauRise=\"0.15ms\"/>\n",
    "    <pulseGenerator id=\"MyStim\" delay=\"10ms\" duration=\"20ms\" amplitude=\"0.5nA\"/>\n",
    "</neuroml>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more tested parameter combinations for alternative cell types.  You can replace the `nmldb_cell_name` above, download a new neuron model, then uncomment and run the corresponding snippet in place of the code above. Or even get a new cell type from other sources, such as [NeuroML-DB]( https://neuroml-db.org/gallery ), [OSB]( https://opensourcebrain.org ) or the internet in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nml_sim/sim_components.nml\n",
    "# <neuroml>\n",
    "#     <include file=\"NMLCL000693/cADpyr232_L5_TTPC2_8bab918b58_0_0.cell.nml\"/>\n",
    "#     <expTwoSynapse id=\"NMDA\" gbase=\"6.5nS\" erev=\"0mV\" tauDecay=\"15ms\" tauRise=\"0.15ms\"/>\n",
    "#     <pulseGenerator id=\"MyStim\" delay=\"10ms\" duration=\"50ms\" amplitude=\"1.5nA\"/>\n",
    "# </neuroml>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nml_sim/sim_components.nml\n",
    "# <neuroml>\n",
    "#     <include file=\"NMLCL001078/dNAC222_L6_SBC_194972ee43_0_0.cell.nml\"/>\n",
    "#     <expTwoSynapse id=\"NMDA\" gbase=\"0.57nS\" erev=\"-0mV\" tauDecay=\"17ms\" tauRise=\"0.05ms\"/>\n",
    "#     <pulseGenerator id=\"MyStim\" delay=\"10ms\" duration=\"100ms\" amplitude=\"0.2nA\"/>\n",
    "# </neuroml>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eden_simulator\n",
    "%time results = eden_simulator.runEden(nml_dir+\"/Sim.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we got out of the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying per-soma activity\n",
    "Let's show how each cell behaved during the simulation, with an analog raster.  \n",
    "\n",
    "We observe that the bottom few cells were stimulated together, fired together and then entered a refractory period; meanwhile the other cells stimulate one another into a wave that spreads out (in spatial order!), reverberates for some time, and dies out near the end of the simulation.  \n",
    "\n",
    "Observe also the sub-threshold behavior; the cells were gradually brought to a depolarisation level before they started firing repetitively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_waveforms = np.array([results[f'{population_name}[{i}]/v'] for i in range(N_cells)])\n",
    "neuron_appearance_order = np.argsort(cell_positions[:,1]) # just in case they weren't sorted before\n",
    "# neuron_appearance_order = range(N_cells) # alternative order, if not sorted\n",
    "fig = plt.figure(figsize=np.array([8,5])*.8); ax = plt.gca()\n",
    "im = plt.imshow(1000*neuron_waveforms[neuron_appearance_order,:], # in mVolts\n",
    "    extent=[ results['t'][0], results['t'][-1], N_cells-.5,-0.5 ],\n",
    "    aspect='auto', interpolation='none', cmap =\"viridis\")\n",
    "cbar = plt.colorbar(im); cbar.set_label('Voltage (mV)')\n",
    "ax.set_xlabel('Time (sec)'); ax.set_ylabel('Neuron #')\n",
    "ax.invert_yaxis() # to match the cell positions' 'up' in 3-D space\n",
    "plt.show(); fig.savefig('tut_net_raster.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an alternative line plot, to see how it gets unwieldy for large population sizes: (it could be ameliorated with a EEG style vertical offset, at the cost of resolution though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(results['t'], neuron_waveforms.T, linewidth=.5)\n",
    "plt.show()\n",
    "fig.savefig('tut_net_jumble.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the cells' positions, we can display soma potential in 3-(4 including time)-D space as well.\n",
    "\n",
    "First, we should reduce the amount of data to display to what's *perceiveable* at the animation speed we choose, i.e. downsample the recorded waveforms in time.  Note that this was already done previously, through the `plt.figure`'s parameters for `figsize` and `dpi`.\n",
    "This is neatly done by the auxiliary [ğšğšğšğš—_ğšœğš’ğš–ğšğš•ğšŠğšğš˜ğš›.ğšğš’ğšœğš™ğš•ğšŠğš¢.ğšŠğš—ğš’ğš–ğšŠğšğš’ğš˜ğš—.ğšœğšğš‹ğšœğšŠğš–ğš™ğš•ğš_ğšğš›ğšŠğš“ğšğšŒğšğš˜ğš›ğš’ğšğšœ]( python_api.rst#eden_simulator.display.animation.subsample_trajectories ) routine, that comes with the [ğšğš’ğšœğš™ğš•ğšŠğš¢]( python_api.rst#display-api ) section of the [Python API]( python_api.rst )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eden_simulator.display.animation import subsample_trajectories\n",
    "samples, anim_axis, sampled_time_axis, [sampled_soma_voltage] = subsample_trajectories(\n",
    "    results['t'], [neuron_waveforms.T * 1000], animation_speed=0.03, animation_frames_per_second=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll display the waveforms as an animated 3D scatter plot, using [ğš”ğŸ¹ğš.ğš™ğš˜ğš’ğš—ğšğšœ]( https://k3d-jupyter.org/reference/factory.points.html ).  We'll also use [ğšğšğšğš—_ğšœğš’ğš–ğšğš•ğšŠğšğš˜ğš›.ğšğš’ğšœğš™ğš•ğšŠğš¢.ğšœğš™ğšŠğšğš’ğšŠğš•.ğš”ğŸ¹ğš.ğ™¿ğš•ğš˜ğš]( python_api.rst#eden_simulator.display.spatial.k3d.plot ) and [ğ™¸ğ™¿ğš¢ğšğš‘ğš˜ğš—.ğšğš’ğšœğš™ğš•ğšŠğš¢.ğ™·ğšƒğ™¼ğ™»]( https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html#IPython.display.HTML ) for improved publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k3d; from eden_simulator.display.spatial.k3d import Plot\n",
    "Point_plot = plot = Plot(camera_auto_fit=False) # to override camera orientation\n",
    "\n",
    "k3d_anim_dict = { str(real_time): x.astype(np.float32) \n",
    "                for (real_time, x) in zip(anim_axis, sampled_soma_voltage)  }\n",
    "k3d_label_dict = { str(real): f't = {sim:.3f} ~ s' for (real, sim) in zip(anim_axis, sampled_time_axis)  }\n",
    "plt_points = k3d.points(positions=cell_positions.astype(np.float32),attribute=k3d_anim_dict,\n",
    "        color_range=[-80, +20],point_size=10,color_map=k3d.matplotlib_color_maps.Rainbow); plot += plt_points\n",
    "\n",
    "plot.camera = plot.get_auto_camera(pitch=30, yaw=10)[:6]+[0,1,0] # set 'y' to up !  LATER zoom a bit also; vecs are pos, tgt, up\n",
    "plt_label = k3d.text2d(k3d_label_dict, (0.,0.), label_box=False); plot += plt_label # add 2d elements AFTER setting auto camera\n",
    "plot.fps = 60;\n",
    "from IPython.display import display, HTML, IFrame\n",
    "plot.snapshot_type = 'inline'; display(HTML(plot.get_snapshot()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "% LATER 2.92 and zoom\n",
    "\\begin{center}\\sphinxincludegraphics[width=0.5\\textwidth]{{_static/tutorial_network_balls}.png}\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this doesn't look much like the tissue we're supposed to simulate, does it?  We have spent all this computer time to simulate all these neurites making up the cell, we deserve more attractive visuals!\n",
    "\n",
    "Let's move on then to display the membrane voltage all over the cell, in glorious, 24 bit, false color! (You'd need fluo to see it on the real thing anyway.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying whole-neuron activity\n",
    "\n",
    "To observe what happens all over the cell, we'll have to *record* all over, and *display* the whole cell.  \n",
    "And we'll achieve this through some cool add-ons to EDEN that: \n",
    "\n",
    "- explain how spatially detailed cells are being simulated as discrete elements of neurite,\n",
    "- and how exactly these parts and the whole neuron look like as polygonal solids.\n",
    "\n",
    "See also the [chapter]( intro_spatial.ipynb ) which explains the structure of spatially-detailed neurons, and introduces and uses said tools.\n",
    "\n",
    "First, we'll retrieve for this cell type, how many *compartments* is is cut into.  Each *compartment* in a model neuron is equivalent to a *pixel* in an image; it is a (hopefully) small bit of the neuron where we assume all electrical, chemical etc. properties are uniform throughout.\n",
    "\n",
    "We can get this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_info = eden_simulator.experimental.explain_cell(nml_dir+\"/Sim.xml\")\n",
    "print(type(cells_info))\n",
    "print(cells_info.keys())\n",
    "cell_info = cells_info[celltype_name]\n",
    "print(cell_info.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each *physically modelled*\\* cell type, we got a set of lists, most with as many elements as there are compartments for each cell type.  We can then use the `comp_midpoint_segment` and `comp_midpoint_fractionAlong` lists to locate the middle of each compartment, and tap into that to record the membrane voltage for each part of each neuron.\n",
    "\n",
    "Given this information, let's record for each cell.  We'll also cut down on the sampling rate (using the Eden-specific extension `<EdenOutputFile>`) because that's way more than the one trajectory per cell we were recording before.\n",
    "\n",
    "To learn more about `explain_cell`, refer to its [Python API page]( python_api.rst#module-eden_simulator.experimental ).\n",
    "\n",
    "\\* that is, excluding artificial cells which typically have unique properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_mid_seg = cell_info['comp_midpoint_segment']\n",
    "comp_mid_fra = cell_info['comp_midpoint_fractionAlong']\n",
    "n_comps = len(comp_mid_seg) # Number of actual compartments in this cell, may be retrieved from most cell_info arrays\n",
    "print(f'{n_comps} compartments per cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll construct a `SimMore.xml` file that records the voltage of *every single compartment* on *every single cell*.  \n",
    "\n",
    "We'll also cut down how often we record these waveforms to 0.5 msec (default NeuroML behaviour is to record for every single timestep which is quite a lot of steps, and we don't always need this much resolution.)\n",
    "\n",
    "Note the use of `<EdenOutputFile>`, an EDEN-specific version of the regular `<OutputFile>` with more [recording options]( extension_io.ipynb#Time-series-with-EdenOutputFile ).  Another one of these is controlling the units per recorded trajectory in `output_units`, which we'll use to record membrane voltage in the more commonly used millivolts, this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a new routine for making the <Simulation> file with new recording options\n",
    "def NmlSimParmss(run_time, run_timestep=25e-6, sampling_period=1e-3,\n",
    "            rec_lines='', href='file://results.gen.txt'):\n",
    "    return f'''\n",
    "        <Simulation id=\"sim1\" length=\"{run_time}s\" step=\"{run_timestep}s\" target=\"Net\">\n",
    "            <EdenOutputFile id=\"first\" href=\"{href}\" format=\"ascii_v0\" sampling_interval=\"{sampling_period} s\">\n",
    "            '''+'\\n\\t        '.join(rec_lines)+'''\n",
    "            </EdenOutputFile>\n",
    "        </Simulation>\n",
    "        <Target component=\"sim1\"/>'''\n",
    "\n",
    "# Make the traces to record each compartment, and save the file\n",
    "traces = [f\"{population_name}[{neu}]/{celltype_name}/{comp_mid_seg[i]}{('%.9f'%comp_mid_fra[i])[1:]}/v\"\n",
    "    for neu in range(N_cells) for i in range(len(comp_mid_seg))]\n",
    "rec_lines = [f'<OutputColumn id=\"v_{i}\" quantity=\"{x}\"  output_units=\"mV\"/>' for i,x in enumerate(traces) ]\n",
    "print(f\"recording {len(rec_lines)} waveforms this time !\")\n",
    "with open(nml_dir+\"/SimMore.xml\", \"w\") as f:   \n",
    "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Lems>\\n<include file=\"example.nml\"/>\\n')\n",
    "    f.write(NmlSimParmss(run_time=0.25, run_timestep=25e-6, sampling_period=0.5e-3, href='./moresults.gen.txt', rec_lines=rec_lines))\n",
    "    f.write('\\n</Lems>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time moresults = eden_simulator.runEden(nml_dir+\"/SimMore.xml\",verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Running simulations with lots of output may often take appreciably more time just to write down and read back all these data ...  \n",
    "... now imagine storing all that, times a whole parameter space D:  \n",
    "(Though this time, we cut down the sampling rate by a lot so it balances out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have many more traces in the sim output, one per neuron per compartment!  \n",
    "\n",
    "Just in case we want differently-shaped neurons in the future, let's leave the traces list to be:\n",
    "\n",
    "- instead of making an array of `N_cells x n_comps x timesteps`, assign an index number in the sequence of traces, from which the set of traces start (n_comps trajectories, including the indexed one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "neuron_waveforms = np.array([moresults[x] for i,x in enumerate(traces)])\n",
    "print('Waveform matrix size:', neuron_waveforms.shape)\n",
    "# XXX append the offsets\n",
    "timevec = moresults['t']\n",
    "offset_per_neuron = np.round(np.arange(N_cells)*n_comps)\n",
    "print('Row start per cell:', offset_per_neuron)\n",
    "im = plt.imshow(neuron_waveforms,\n",
    "    extent=[ results['t'][0], timevec[-1], N_cells-.5,-0.5 ],\n",
    "    aspect='auto', interpolation='none', cmap =\"viridis\");cbar = plt.colorbar(im); cbar.set_label('Voltage (mV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ran `explain_cell`, some lists `mesh_vertices`, `mesh_faces` and `mesh_comp_per_face` were also provided for the cell.  These represent the solid shape (in computer graphics parlance, a *mesh*) that cells of this type have. (For unique morphology, one would make individual cell types in NeuroML.)\n",
    "\n",
    "*Meshes* are made up from a set of points (known as *vertices*) in 3-D space, and a set of polygonal *faces* (typically triangles).  Thus the joined flat *faces* form the shape together, just like lines connected through lines do in 2D space.  To display a 3-D object, meshes may be enhanced with texture images and related attributes, to show a more detailed surface on the meshes.  But for our purpose, we'll be painting the mesh explicitly, so that we can show biophysical attributes across each cell.\n",
    "\n",
    "Here is what the neuron looks like according to the provided mesh, in plain color:  \n",
    "(Use the mouse over the picture to rotate/move the neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_vertices = cell_info['mesh_vertices']\n",
    "mesh_faces    = cell_info['mesh_faces'   ]\n",
    "import trimesh\n",
    "viz = trimesh.Trimesh(\n",
    "    vertices=mesh_vertices, faces=mesh_faces )\n",
    "viz.visual.face_colors = (0.1,0.9,0.1)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "% LATER automate\n",
    "\\begin{center}\\sphinxincludegraphics[width=0.9\\textwidth]{{_static/tutorial_network_neuron}.png}\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that that if $z$ is up, that's a different orientation compared to the [render](https://neuroml-db.org/api/gif?id=NMLCL000625) in the NeuroML-DB.\n",
    "\n",
    "For the following, we'll assume $\\,\\overrightarrow y~$ points to \"up\" and rotate the `plot.camera` accordingly, just like we did in the previous point-based animation.  (For more about the meaning of each coordinate axis, refer to the [article]( intro_spatial.ipynb#Standards-in-morphology-coordinates ) on spatially-detailed cells.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the `mesh`'s `vertices` and `faces` we got a list `mesh_comp_per_face`, which indicates which *compartment* is represented by each *face* of the mesh.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_comp = cell_info['mesh_comp_per_face']\n",
    "print(f\"We have {len(mesh_faces)} faces in the mesh, and {len(face_comp)} matching labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this with the vertex numbers that each face is made of, we can have both a mapping from each compartment to each corresponding face, as well as from each compartment to each vertex. (Different 3D graphics utilities prefer each form.)\n",
    "\n",
    "We'll now use this mapping to paint the neuron selectively, with a different colour for each compartment, to show how membrane potential spreads when it's initiated from the soma.  This can be done immediately through the auxiliary [ğšğšğšğš—_ğšœğš’ğš–ğšğš•ğšŠğšğš˜ğš›.ğšğš’ğšœğš™ğš•ğšŠğš¢.ğšœğš™ğšŠğšğš’ğšŠğš•.ğš”ğŸ¹ğš.ğš™ğš•ğš˜ğš_ğš—ğšğšğš›ğš˜ğš—]( python_api.rst#eden_simulator.display.spatial.k3d.plot_neuron ) routine, that comes with the [ğšğš’ğšœğš™ğš•ğšŠğš¢]( python_api.rst#display-api ) section of the [Python API]( python_api.rst ).\n",
    "\n",
    "**Tip:** When animating a spatially neuron over many time samples, use `compress_cells = True`(default) with `plot_neuron` and display it on a `eden_simulator.display.spatial.k3d.Plot` with `IPython.display.HTML` or `IPython.display.IFrame` to keep the memory and space requirements under control. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eden_simulator.display.animation import subsample_trajectories\n",
    "_, anim_axis, sampled_time_axis_sec, (sampled_voltage,) = subsample_trajectories(\n",
    "    timevec, [neuron_waveforms.T], animation_speed=0.03, animation_frames_per_second=60)\n",
    "\n",
    "import k3d; from eden_simulator.display.spatial.k3d import Plot, plot_neuron\n",
    "Multicomp_plot = plot = Plot()\n",
    "k3d_label_dict = { str(real): f't = {sim:.3f} ~ s' for (real, sim) in zip(anim_axis, sampled_time_axis_sec)  }\n",
    "for neu in range(N_cells):\n",
    "    tra_off = offset_per_neuron[neu] # there some traces per neuron, starting from ...\n",
    "    comp_trajes = sampled_voltage[:,tra_off:tra_off+n_comps] # there they are\n",
    "    plot += plot_neuron(cell_info, comp_trajes, time_axis_sec=anim_axis, translation=cell_positions[neu,:],\n",
    "                            color_range=[-80, 0],color_map='rainbow');\n",
    "plot.camera = plot.get_auto_camera(pitch=30, yaw=10)[:6]+[0,1,0] # set 'y' to up !  LATER zoom a bit also; vecs are pos, tgt, up\n",
    "plt_label = k3d.text2d(k3d_label_dict, (0.,0.)); plot += plt_label # add 2d elements AFTER setting auto camera\n",
    "\n",
    "# Since the plot is quite big, include it outside of the notebook so that the notebook renders quick.\n",
    "plot.snapshot_type = 'online'; plot_html = plot.get_snapshot()\n",
    "plot_file = '_static/tutorial_network_big_animation.html'\n",
    "with open(plot_file, 'w') as f: f.write(plot_html)\n",
    "display(IFrame('_static/tutorial_network_big_animation.html', '100%', f'{plot.height} px'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/latex"
   },
   "source": [
    "% 3.30483257588077 0.099s\n",
    "\\begin{center}\\sphinxincludegraphics[width=0.9\\textwidth]{{_static/tutorial_network_detailed}.png}\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Although the animation runs over several seconds, neurons fire quite fast; Use the `Play/Stop loop` button and the 'time' slider in `Controls`, to see -->\n",
    "Observe how the spikes travel along the neurites.\n",
    "\n",
    "*Exercise*: While constructing the model (before implementing the NeuroML desctiption), there was a mistake in one of the formulae for the network parameters.  This mistake makes whether the neurons' spatially reverberating waves occur, extremely sensitive to the physical parameters of the neuron model, synapses and applied probes.  \n",
    "\n",
    "* Find the mistake and re-evaluate sensitivity (also with the alternative cell types commented out [above]( #Generating-NeuroML-for-the-model )) using the corrected, plausible formula.  If a fine-tuned, *wrong* model can reproduce the expected phenomenon, what does that imply for computational neuroscience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "Now as the last thing, we'll fetch a screenshot to use in the documentation's [example gallery]( https://eden-simulator.org/gallery.html ).  \n",
    "We'll do this here by re-doing the whole plot, because it's too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Selenized browsers crash with BUFFER_SHORTAGE when the whole plot is loaded in one go :(\n",
    "# Thus: Re-make the animation data and render frame by frame\n",
    "lotp = k3d.plot(grid_visible=False,screenshot_scale=1,axes_helper=0,camera_auto_fit=False) # to override camera orientation\n",
    "lotp.colorbar_object_id = 0 # Disabling colorLegend programmatically\n",
    "\n",
    "# LATER use subsampled values.\n",
    "data_timestep = np.diff(timevec)[0] # assuming fixed timestep\n",
    "traj_samples_per_frame = round((0.03/60)/data_timestep)\n",
    "sample_for_frame = range(int(0.1/data_timestep),int(0.103/data_timestep),traj_samples_per_frame)\n",
    "\n",
    "neuron_meshes = [None] * N_cells\n",
    "frames = []\n",
    "\n",
    "mapped_verts, mapped_faces, verts_per_comp, faces_per_comp = eden_simulator.display.spatial.get_verts_faces_per_comp(mesh_vertices, mesh_faces, face_comp, n_comps)\n",
    "# LATER start and generator\n",
    "# LATER gridautofit, gridvisible ...\n",
    "import time\n",
    "try:\n",
    "    from k3d.headless import k3d_remote, get_headless_driver\n",
    "    headless = k3d_remote(lotp, get_headless_driver(), width=500, height=500)\n",
    "    for neu in range(N_cells):\n",
    "        plt_mesh = k3d.mesh(mesh_vertices+cell_positions[neu,:].astype('float32'), mesh_faces.astype('uint32'),\n",
    "            attribute=[99], color_range=[-80, 0], color_map=k3d.matplotlib_color_maps.Rainbow); lotp += plt_mesh\n",
    "        neuron_meshes[neu] = plt_mesh\n",
    "    lotp.camera = lotp.get_auto_camera(pitch=30, yaw=10)[:6]+[0,1,0] # set 'y' to up !  LATER zoom a bit also; vecs are pos, tgt, up\n",
    "    # lotp.camera = [-45.96106410442951, 178.83172993394766, 242.69050004118378, 163.18205388309028, -58.73859569108522, -262.78073580267176, 0,1,0]\n",
    "    headless.sync(hold_until_refreshed=True)\n",
    "    headless.camera_reset(.6)\n",
    "    for frame,sample in enumerate(sample_for_frame):\n",
    "        real_time = real_time = frame * (1/60)# real_times[frame]\n",
    "        for neu in range(N_cells):\n",
    "            tra_off = offset_per_neuron[neu] # there some traces per neuron, starting from ...\n",
    "            comp_trajes = neuron_waveforms[tra_off:tra_off+n_comps,:] # there they are\n",
    "            real_time = frame * (1/60)\n",
    "            sim_time  = timevec[sample]#frame * sim_sec_per_frame\n",
    "            # for each vertex; units are already set to mVolt in EdenOutputFile !\n",
    "            neuron_meshes[neu].attribute = (verts_per_comp.T @ comp_trajes[:,sample]).astype('float32')\n",
    "        # lotp.time = real_time\n",
    "        lotp.camera[0]+=10\n",
    "        headless.sync()\n",
    "        time_start = time.time()\n",
    "        screenhot = headless.get_screenshot()\n",
    "        time_end = time.time()\n",
    "        # print(f\"{time_end - time_start} sec\")\n",
    "        frames.append(screenhot)\n",
    "    lotp.close()\n",
    "finally:\n",
    "    headless.close()\n",
    "import IPython\n",
    "IPython.display.Image(data=frames[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Save a full render for the Latex version LATER.\n",
    "# with open('_static/tutorial_network_screenshot_full.png','wb') as f: f.write(frames[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "\n",
    "# Stage 1: Crop\n",
    "frame_data = [ Image.open(io.BytesIO(x)) for x in frames]\n",
    "frame_data = [ x.crop((int(0.25*x.size[0]), 0, x.size[0], x.size[1])) for x in frame_data ]\n",
    "# frame_data = [ x.crop((int(0.25*x.size[0]), int(0.25*x.size[1]), x.size[0], x.size[1])) for x in frame_data ]\n",
    "\n",
    "# Stage 2: Resize\n",
    "for x in frame_data: x.thumbnail((240,200))\n",
    "    \n",
    "# Stage 3: Lossy - or not\n",
    "# frame_data = [x.convert('P',palette=Image.Palette.ADAPTIVE,dither=Image.Dither.NONE,colors=256) for x in frame_data]\n",
    "# to apng, or gif\n",
    "outbuf = io.BytesIO()\n",
    "frame_data[0].save(outbuf, format='gif', save_all=True, append_images=frame_data[1:], duration=100, loop=0)\n",
    "print('GIF size:',len(outbuf.getvalue()))\n",
    "\n",
    "# to alpha channel out - how is this not compressed already\n",
    "im = Image.open(outbuf)\n",
    "# https://github.com/python-pillow/Pillow/issues/3292#issuecomment-410837926\n",
    "newframes = [x.copy().convert('RGB') for x in PIL.ImageSequence.Iterator(im)]\n",
    "\n",
    "aabuf = io.BytesIO()\n",
    "newframes[0].save(aabuf, format='png', save_all=True, append_images=newframes[1:], duration=100, loop=0)\n",
    "print(len(aabuf.getvalue()))\n",
    "\n",
    "# to lossless recompression\n",
    "import oxipng\n",
    "oxipng_opts = {'level':6}\n",
    "aa = oxipng.optimize_from_memory(aabuf.getvalue(), **oxipng_opts)\n",
    "print('Optimized PNG size:',len(aa))\n",
    "\n",
    "display(IPython.display.Image(data=outbuf.getvalue()))\n",
    "# display(IPython.display.Image(data=aabuf.getvalue()))\n",
    "# display(IPython.display.Image(data=aa))\n",
    "\n",
    "# IPython refuses to nbconvert gif files, and apng is not as efficient, we'll have to get creative ...\n",
    "# Accept having a png file for now? nah it's twice as big as the gif after gif compression.\n",
    "# and save the file under an explicit name bc nbsphinx is misbehaving\n",
    "# with open('_static/thumb_tut_net.png','wb') as f: f.write(aa)\n",
    "with open('_static/thumb_tut_net.gif','wb') as f: f.write(outbuf.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "And minimize plots for publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "from eden_simulator.display.spatial.k3d import MinimizePlot\n",
    "for x in [Point_plot, Multicomp_plot, lotp]: MinimizePlot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "And after the last thing, move the generated figures to their right place and clean up after this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "# check if inside the docs or not\n",
    "if os.path.exists('conf.py'): basedir = '.'\n",
    "else: basedir = './docs/' # assume we cloned in repo root, later reconsider cwd'ing earlier on...\n",
    "\n",
    "for x in ['tut_net_raster.png','tut_net_jumble.png']:\n",
    "    shutil.move(x,f'{basedir}/_static/{x}')\n",
    "\n",
    "shutil.rmtree('tut_net', ignore_errors=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
