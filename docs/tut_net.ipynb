{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: A network of detailed cells\n",
    "<span id=\"top\" />\n",
    "This tutorial shows how to build an active network of detailed neurons, simulate it to get the neurons' potentials over time in full spatial detail, and display these data as an interactive 3D animation, or a rendered movie file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q 'scipy>=1.12' # for an evenly spread, *quasi* pseudo random distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the network\n",
    "\n",
    "Building a network model generally incolves the following actions:\n",
    "- place the *cells* forming the network;\n",
    "- connect the cells with *synapses*;\n",
    "- add the experimental *rig*: add external stimuli and probe the electro-chemical variables of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing neurons\n",
    "First, let's decide where to place the neurons in the model.  For this tutorial, assume that neurons are uniformly spread over, say a tall cylinder that stands for a microcolumn.  We'll use an evenly spread [*quasi*-pseudo-random distribution](https://en.wikipedia.org/wiki/Low-discrepancy_sequence), which won't randomly form misleading clumps like a true random sample would. We'll also sort the cells by distance along the cylinder, to give more  meaning to the the resulting rasters and correlation matrices.\n",
    "\n",
    "Check the following code also for a neat way to get uniformly random points over an arbitrary shape:\n",
    "- check they fall on the shape, keep only those who do.\n",
    "- resample to fill in the remaining slots.\n",
    "\n",
    "Kind of like painting with a stencil :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a few cells\n",
    "# randomize x, y, z using e.g. a cylinder\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.qmc.Halton.html#scipy.stats.qmc.Halton\n",
    "seed = 123\n",
    "N_cells = 30\n",
    "# Points will be placed in a cylinder of given radius and height\n",
    "region_radius = 50 # microns\n",
    "region_height = 200\n",
    "\n",
    "from scipy.stats import qmc\n",
    "position_rng = qmc.Halton(d=3, scramble=True, seed=seed)\n",
    "\n",
    "# Use rejection, to make the 3D points conform to an arbitrary domain\n",
    "remaining_cells = N_cells\n",
    "cell_positions = []\n",
    "while len(cell_positions) < N_cells:\n",
    "    # Get points over a uniform box\n",
    "    point_samples = (\n",
    "        position_rng.random(n=N_cells-len(cell_positions)) \n",
    "        * np.array([2*region_radius,region_height,2*region_radius]) \n",
    "        - np.array([region_radius,0,region_radius]) #list()\n",
    "    )\n",
    "    valid_points = [ (x,y,z) for (x,y,z) in point_samples if (x**2+z**2<region_radius**2) and (y > 0 and y < region_height) ]\n",
    "    cell_positions += valid_points\n",
    "cell_positions = np.array(cell_positions)\n",
    "\n",
    "# Also! Sort by height to make our lives easier!\n",
    "cell_positions = cell_positions[np.argsort(cell_positions[:,1])]\n",
    "# cell_positions is now ready!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the distribution visually. If running the notebook, rotate the display to see the distribution from all sides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure(1); fig.clear(); fig.set_size_inches(4,5)\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "# Note: usually in SWC and NeuroML files, +Y is 'up', zero is the soma middle, and units are in microns.\n",
    "# But these also vary with the provenance of the files, always check before using.\n",
    "# Swap Y and Z for viz purposes.\n",
    "for i, (x, z, y) in enumerate(cell_positions):\n",
    "    ax.text(x, y, z, f'{i}', color='green')\n",
    "# Tweaking display region and labels\n",
    "ax.set_xlim(-region_radius,+region_radius)\n",
    "ax.set_ylim(-region_radius,+region_radius)\n",
    "ax.set_zlim(0, region_height)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel('Width (μm)'); ax.set_ylabel('Depth (μm)'); ax.set_zlabel('Height (μm)')\n",
    "ax.view_init(elev=9, azim=-50, roll=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding synapses\n",
    "Now let's hook up the neurons with synapses that will cause the cells to [stimulate]( https://en.wikipedia.org/wiki/Excitatory_postsynaptic_potential ) each other, creating an interesting effect. To keep the code simple, we'll consider connections over each pair of cells (it's fine if there are not thousands of them).\n",
    "\n",
    "We will assume *distance-dependent* _probability_ (of each pair-wise synapse forming at all), _weight_ (slightly randomized) and _delay_ (linear with distance). Check the code for the specific formulae.\n",
    "\n",
    "For this tutorial, we'll only consider soma-to-soma connections; feel free to use your preferred models and methods.\n",
    "<!-- TODO point to comp. breakdown ...  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import default_rng\n",
    "synapses_seed = 123\n",
    "rng = default_rng(synapses_seed)\n",
    "\n",
    "# The range constant (sigma, in this case) for the synapses.\n",
    "syn_radius = 100\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "d_mat = squareform(pdist(cell_positions)) # Distance between all pairs of cell somata\n",
    "p_mat = 0.7*np.exp(-(d_mat/syn_radius)**2) # Chance for connection = f(distance)\n",
    "p_mat = p_mat - np.diag(np.diag(p_mat)) # Remove the diagonal\n",
    "\n",
    "w_mat = 0.5*(rng.standard_normal(d_mat.shape)+2) * np.exp(-(d_mat/syn_radius)**2) # Variable weight between pairs\n",
    "t_mat = 0.001*np.exp(-(d_mat/syn_radius)**2) # Deterministic delay between pairs, in seconds\n",
    "\n",
    "pre = []; post = []; weight = []; delay = []; # Parallel pre/post synaptic cell, weight, delay for ...\n",
    "for pre_cell in range(N_cells):\n",
    "    post_cells = (np.argwhere(p_mat[:,pre_cell] > rng.random(N_cells))).flatten() # Get synapse targets that pass the Bernoulli test\n",
    "#     print(post_cells)\n",
    "    pre += ([pre_cell] * len(post_cells));  # append the relevant syn pairs, with weight and delay\n",
    "    post.extend(post_cells);\n",
    "    weight.extend(w_mat[pre_cell,post_cells]);\n",
    "    delay.extend(t_mat[pre_cell,post_cells]);\n",
    "# print( \"from:\", pre, \"\\nto  :\", post, \"\\nwei :\", weight, \"\\ndel :\", delay )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show the per-neuron-pair matrices of the factors involved.\n",
    "\n",
    "First, tabulate the absolute distance, and the probability of a connection for each neuron pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.close('all')\n",
    "axs = plt.figure(figsize=(11,4)).subplots(nrows=1, ncols=2);\n",
    "axs[0].set_title('Distance (μm)'); im = axs[0].imshow(d_mat, cmap='Reds'); fig.colorbar(im, ax=axs[0]);\n",
    "axs[1].set_title('P(Syn)'); im = axs[1].imshow(p_mat, cmap='Blues'); fig.colorbar(im, ax=axs[1]);\n",
    "for i in range(axs.shape[0]): axs[i].set_ylabel('From neuron'); axs[i].set_xlabel('To neuron');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, tabulate weight and delay for the connections that were actually realized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_2d = np.zeros((N_cells,N_cells))*np.nan; d_2d = w_2d + 0 # full mask\n",
    "w_2d[pre,post] = weight; d_2d[pre,post] = delay # fill in the values that apply\n",
    "\n",
    "axs = plt.figure(figsize=(11,4)).subplots(nrows=1, ncols=2);\n",
    "axs[0].set_title('Weight'); im = axs[0].imshow(w_2d, cmap='YlOrBr'); fig.colorbar(im, ax=axs[0]); # TODO use a diverging cmap centered in 0, ...\n",
    "axs[1].set_title('Delay (msec)'); im = axs[1].imshow(d_2d*1000, cmap='Greens'); fig.colorbar(im, ax=axs[1]);\n",
    "for i in range(axs.shape[0]): axs[i].set_ylabel('From neuron'); axs[i].set_xlabel('To neuron');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding stimuli\n",
    "\n",
    "Now let's play with the network through unnatural means -- for example, apply a one-off DC clamp to cells in the bottom 20% of the cylinder.\n",
    "\n",
    "The attentive reader may have noticed that what's been specified so far is not specific to NeuroML.  That's because NeuroML can run any *distribution* of cells and synapses just the same; they can be constructed independently with any method, and expressed in the NeuroML data format just before runnimg the model.\n",
    "\n",
    "The more involved and detailed descriptions(neuron shape, chemistry, dynamics and such) for the individual parts of the model will be specified in NeuroML, in the section right after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_cells = [i for i, (x,y,z) in enumerate(cell_positions) if y < region_height/5]\n",
    "stim_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating NeuroML for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to make the NeuroML files in order to run the simulation.\n",
    "Let's put them in a sub-folder of the working directory, to avoid polluting the folder that the notebook is in too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nml_dir = 'tut_net/'\n",
    "!mkdir -p $nml_dir #TODO for windows as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cell's description, let's cheat and grab an existing description from the NeuroML-DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmldb_cell_name = 'NMLCL000625'\n",
    "zip_file_name = f'{nmldb_cell_name}.zip'\n",
    "# Download the zip file with the model\n",
    "import urllib.request\n",
    "# Because NMLDB is hoving some issues with HTTPS, don't demand HTTPS verification\n",
    "import ssl; ssl._create_default_https_context = ssl._create_unverified_context\n",
    "urllib.request.urlretrieve(f'http://neuroml-db.org/GetModelZip?modelID={nmldb_cell_name}&version=NeuroML', zip_file_name)\n",
    "# and unpack it\n",
    "from zipfile import ZipFile\n",
    "with ZipFile(zip_file_name, 'r') as zipp: zipp.extractall(nml_dir+nmldb_cell_name+'/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need the cell type's identifier from the NML file, which for some reason is not the `NMLCLxxxxxx` identifier. (Neither is the NML file)  \n",
    "This is the same as the `<name>.cell.nml` filename, so let's scan for that.  \n",
    "Also, the `<spikeThresh>` to register an action potential is not specified for some reason.  Because we're using classical chemical synapses, we'll have to add it to `<biophysicalProperties>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATER get it with libNML or with the NMLDB API.\n",
    "import os\n",
    "celltype_name = None\n",
    "cell_nml_file_suffix = '.cell.nml'\n",
    "for filename in os.listdir(nml_dir+nmldb_cell_name):\n",
    "    # print(filename)\n",
    "    if not filename.endswith(cell_nml_file_suffix): continue # get the cell.nml files\n",
    "    new_celltype_name = filename[:-len(cell_nml_file_suffix)] # there it is\n",
    "    if celltype_name: raise ValueError(f'cell type: is it {new_celltype_name} or {new_celltype_name}') # :c\n",
    "    celltype_name = new_celltype_name\n",
    "print('Celltype name:', celltype_name)\n",
    "\n",
    "# Also modify the NML file to add spikeThresh\n",
    "cell_filename = nml_dir+nmldb_cell_name+'/'+new_celltype_name+cell_nml_file_suffix\n",
    "# Read in the file\n",
    "with open(cell_filename, 'r') as file: filedata = file.read()\n",
    "# Replace the target string\n",
    "if '<spikeThresh' not in filedata:filedata = filedata.replace('</membraneProperties>','<spikeThresh value=\"0 mV\"/></membraneProperties>')\n",
    "# Write the file out again\n",
    "with open(cell_filename, 'w') as file: file.write(filedata)\n",
    "# print(filedata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add some routines to construct a NeuroML file incrementally from the population and projection data we collected so far, into one big string.\n",
    "There are other ways to create and manipulate [TODO NeuroML refrence and external resources].  But this one here makes for a more direct demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NmlHeader():\n",
    "    return '''<neuroml xmlns=\"http://www.neuroml.org/schema/neuroml2\"  xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.neuroml.org/schema/neuroml2 https://raw.github.com/NeuroML/NeuroML2/development/Schemas/NeuroML2/NeuroML_v2.1.xsd\">\n",
    "    <include file=\"sim_components.nml\"/>'''\n",
    "def NmlNetworkHeader(): return '''\\n   <network id=\"Net\" type=\"networkWithTemperature\" temperature=\"37degC\">'''\n",
    "def NmlNetworkFooter(): return '''\\n    </network>'''\n",
    "\n",
    "def NmlPopulation(pop_name, celltype_name, cell_positions):\n",
    "    return f'''\n",
    "        <population id=\"{pop_name}\" component=\"{celltype_name}\" size=\"{len(cell_positions)}\">\n",
    "        '''+'\\n\\t'.join([\n",
    "            f'  <instance id=\"{i}\"><location x=\"{x}\" y=\"{y}\" z=\"{z}\"/></instance>'\n",
    "            for i, (x,y,z) in enumerate(cell_positions) ])+'''\n",
    "        </population>\n",
    "    '''\n",
    "\n",
    "def NmlSynapticProjection(\n",
    "    proj_name,syncomp_name, pre_pop_name, post_pop_name,\n",
    "    preCell, postCell, weight=None, delay=None, preSeg=None, postSeg = None, preFrac = None, postFrac = None):\n",
    "    if delay is None: delay = [0]*len(preCell)\n",
    "    if weight is None: weight = [1]*len(preCell)\n",
    "    if preSeg is None: preSeg = [0]*len(preCell)\n",
    "    if postSeg is None: postSeg = [0]*len(preCell)\n",
    "    if preFrac is None: preFrac = [0]*len(preCell)\n",
    "    if postFrac is None: postFrac = [0]*len(preCell)\n",
    "    \n",
    "    return f'''\n",
    "        <projection id=\"{proj_name}\" synapse=\"{syncomp_name}\" presynapticPopulation=\"{pre_pop_name}\" postsynapticPopulation=\"{post_pop_name}\">\n",
    "            '''+'\\n\\t    '.join([\n",
    "            f'<connectionWD id=\"{i}\" preCellId=\"{preCell[i]}\" postCellId=\"{postCell[i]}\" '+\n",
    "            f'weight=\"{weight[i]}\" delay=\"{delay[i]} s\" '+\n",
    "            f'preSegmentId=\"{preSeg[i]}\" postSegmentId=\"{postSeg[i]}\" '+\n",
    "            f'preFractionAlong=\"{preFrac[i]}\" postFractionAlong=\"{postFrac[i]}\"/>'\n",
    "            for i in range(len(pre)) ])+'''\n",
    "        </projection>\n",
    "    '''\n",
    "    \n",
    "def NmlInputList(input_list_name,stim_component,target_pop_name,cells, segs=None, fracs=None):\n",
    "    if segs is None: segs = [0]*len(cells)\n",
    "    if fracs is None: fracs = [0]*len(cells)\n",
    "    return f'''\n",
    "        <inputList id=\"{input_list_name}\" population=\"{target_pop_name}\" component=\"{stim_component}\">\n",
    "        '''+'\\n\\t'.join([\n",
    "            f'  <input id=\"{i}\" target=\"../{target_pop_name}[{cells[i]}]\" '+\n",
    "            f'segmentId=\"{segs[i]}\" fractionAlong=\"{fracs[i]}\" destination=\"synapses\"/>'\n",
    "            for i in range(len(cells)) ])+'''\n",
    "        </inputList>\n",
    "    '''\n",
    "\n",
    "def NmlFooter():return '''\\n</neuroml>'''\n",
    "\n",
    "# celltype_name = 'cACint209_L6_NBC_a3972c5d97_0_0'\n",
    "# celltype_name = 'cADpyr232_L5_TTPC2_8bab918b58_0_0'\n",
    "# celltype_name = 'dNAC222_L6_SBC_194972ee43_0_0'\n",
    "\n",
    "population_name = 'MyCells'\n",
    "\n",
    "# Write the network file.\n",
    "with open(nml_dir+\"/example.nml\", \"w\") as f:\n",
    "    f.write(NmlHeader()); f.write(NmlNetworkHeader())\n",
    "    f.write(NmlPopulation(population_name, celltype_name, cell_positions))\n",
    "    f.write(NmlSynapticProjection('FirstSynProjection', 'NMDA', population_name, population_name, pre, post, weight, delay))\n",
    "    f.write(NmlInputList('FirstStimList', 'MyStim', population_name, stim_cells))\n",
    "    f.write(NmlNetworkFooter())\n",
    "    f.write(NmlFooter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After writing the `net.nml` representing the network made up of cells, synapses and input stimuli, we'll make a companion file \n",
    "LEMS_<something>.xml (this is the convention; I guess you could also use `.sim.xml` to make the name further clearer.)\n",
    "\n",
    "Here, we'll specify some parameters to run the simulation like for how long and with how big a timestep, but also add the last part of the rig: recording certain trajectories and spike trains from the simulated model. Since neurites in each cell usually fire togher(in sequence), we'll record fthe membrane voltage trajectories for the soma of each neuron.  (Conveniently enough, if the location on the neuron is not specified in a NeuroML path, the location of the soma is used.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sim file.\n",
    "def NmlSimParms(pop_name, N_cells, run_time, run_timestep=25e-6):\n",
    "    return f'''\n",
    "        <Simulation id=\"sim1\" length=\"{run_time}s\" step=\"{run_timestep}s\" target=\"Net\">\n",
    "            <OutputFile id=\"first\" fileName=\"tut_net/results.gen.txt\">\n",
    "            '''+'\\n\\t        '.join([f'<OutputColumn id=\"v_{i}\" quantity=\"{pop_name}[{i}]/v\"/>' for i in range(N_cells)])+'''\n",
    "            </OutputFile>\n",
    "        </Simulation>\n",
    "        <Target component=\"sim1\"/>'''\n",
    "\n",
    "with open(nml_dir+\"/Sim.xml\", \"w\") as f:   \n",
    "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Lems>\\n<include file=\"example.nml\"/>\\n')\n",
    "    f.write(NmlSimParms(population_name, N_cells, run_time=0.25, run_timestep=25e-6))\n",
    "#     f.write(NmlSimParmss(population_name,celltype_name,None, run_time=0.25, run_timestep=25e-6))\n",
    "    f.write('\\n</Lems>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $nml_dir/sim_components.nml\n",
    "<neuroml>\n",
    "    <include file=\"NMLCL000625/cACint209_L6_NBC_a3972c5d97_0_0.cell.nml\"/>\n",
    "    <expTwoSynapse id=\"NMDA\" gbase=\".5nS\" erev=\"0mV\" tauDecay=\"15ms\" tauRise=\"0.15ms\"/>\n",
    "    <pulseGenerator id=\"MyStim\" delay=\"10ms\" duration=\"20ms\" amplitude=\"0.5nA\"/>\n",
    "</neuroml>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some more tested parameter combinations for alternative cell types.  You can replace the `nmldb_cell_name` above, download a new neuron model, then uncomment and run the corresponding snippet in place of the code above. Or even get a new cell type from other sources, such as [NeuroML-DB]( https://neuroml-db.org/gallery ), [OSB]( https://opensourcebrain.org ) or the internet in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nml_sim/sim_components.nml\n",
    "# <neuroml>\n",
    "#     <include file=\"NMLCL000693/cADpyr232_L5_TTPC2_8bab918b58_0_0.cell.nml\"/>\n",
    "#     <expTwoSynapse id=\"NMDA\" gbase=\"6.5nS\" erev=\"0mV\" tauDecay=\"15ms\" tauRise=\"0.15ms\"/>\n",
    "#     <pulseGenerator id=\"MyStim\" delay=\"10ms\" duration=\"50ms\" amplitude=\"1.5nA\"/>\n",
    "# </neuroml>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile nml_sim/sim_components.nml\n",
    "# <neuroml>\n",
    "#     <include file=\"NMLCL001078/dNAC222_L6_SBC_194972ee43_0_0.cell.nml\"/>\n",
    "#     <expTwoSynapse id=\"NMDA\" gbase=\"0.57nS\" erev=\"-0mV\" tauDecay=\"17ms\" tauRise=\"0.05ms\"/>\n",
    "#     <pulseGenerator id=\"MyStim\" delay=\"10ms\" duration=\"100ms\" amplitude=\"0.2nA\"/>\n",
    "# </neuroml>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eden_simulator\n",
    "%time results = eden_simulator.runEden(nml_dir+\"/Sim.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we got out of the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying per-soma activity\n",
    "Let's show how each cell behaved during the simulation, with an analog raster.  \n",
    "\n",
    "We observe that the bottom few cells were stimulated together, fired together and then entered a refractory period; meanwhile the other cells stimulate one another into a wave that spreads out (in spatial order!), reverberates for some time, and dies out near the end of the simulation.  \n",
    "\n",
    "Observe also the sub-threshold behavior; the cells were gradually brought to a depolarisation level before they started firing repetitively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_waveforms = np.array([results[f'{population_name}[{i}]/v'] for i in range(N_cells)])\n",
    "neuron_appearance_order = np.argsort(cell_positions[:,1]) # just in case they weren't sorted before\n",
    "# neuron_appearance_order = range(N_cells) # alternative order, if not sorted\n",
    "fig = plt.figure(figsize=np.array([8,5])*.8); ax = plt.gca()\n",
    "im = plt.imshow(1000*neuron_waveforms[neuron_appearance_order,:], # in mVolts\n",
    "    extent=[ results['t'][0], results['t'][-1], N_cells-.5,-0.5 ],\n",
    "    aspect='auto', interpolation='none', cmap =\"viridis\")\n",
    "cbar = plt.colorbar(im); cbar.set_label('Voltage (mV)')\n",
    "ax.set_xlabel('Time (sec)'); ax.set_ylabel('Neuron #')\n",
    "ax.invert_yaxis() # to match the cell positions' 'up' in 3-D space\n",
    "plt.show(); fig.savefig('_images/tut_net_raster.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an alternative line plot, to see how it gets unwieldy for large population sizes: (it could be ameliorated with a EEG style vertical offset, at the cost of resolution though)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(results['t'], neuron_waveforms.T, linewidth=.5)\n",
    "plt.show()\n",
    "fig.savefig('_static/tut_net_jumble.png',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the cells' positions, we can display soma potential in 3-(4 including time)-D space as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make the animation data\n",
    "timevec = results['t']\n",
    "sim_timestep = np.diff(timevec)[0] # assuming fixed timestep\n",
    "anim_fps = 60; real_sec_per_frame = 1/anim_fps # for animated display\n",
    "sim_sec_per_frame = 0.5e-3 # for sampling the data\n",
    "traj_samples_per_frame = round(sim_sec_per_frame/sim_timestep)\n",
    "sample_for_frame = range(0,len(timevec),traj_samples_per_frame)\n",
    "print(\"Animation duration: %d frames, %.3f sec\"\n",
    "      % (len(sample_for_frame), len(sample_for_frame)*real_sec_per_frame))\n",
    "\n",
    "k3d_anim_dict = {}; k3d_label_dict = {} # real time in sec string, to values \n",
    "for frame,sample in enumerate(sample_for_frame):\n",
    "    real_time = frame * real_sec_per_frame\n",
    "    sim_time  = frame * sim_sec_per_frame\n",
    "    \n",
    "    k3d_anim_dict[str(real_time)] = neuron_waveforms[:,sample] * 1000 # to mVolt\n",
    "    k3d_label_dict[str(real_time)] = f't = {sim_time:.3f} s'\n",
    "# print(k3d_anim_dict.keys())\n",
    "# and finally, show it!\n",
    "import k3d; from k3d.colormaps import matplotlib_color_maps as k3dmaps\n",
    "plot = k3d.plot(camera_auto_fit=False) # to override camera orientation\n",
    "plt_points = k3d.points(positions=cell_positions,point_size=10,\n",
    "    attribute=k3d_anim_dict, color_range=[-80, +20], color_map=k3dmaps.Rainbow); plot += plt_points\n",
    "plot.camera = plot.get_auto_camera(pitch=30, yaw=10)[:6]+[0,1,0] # set 'y' to up !  TODO zoom a bit also; vecs are pos, tgt, up\n",
    "plt_label = k3d.text2d(k3d_label_dict, (0.,0.)); plot += plt_label # add 2d elements AFTER setting auto camera\n",
    "# plot.display(); \n",
    "plot.fps = anim_fps;# plot.start_auto_play()\n",
    "from IPython.display import display, HTML\n",
    "plot.snapshot_type = 'inline'\n",
    "HTML(plot.get_snapshot()) # additional_js_code='K3DInstance.startAutoPlay();')\n",
    "Point_plot = plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On your right there is a control menu. Click on `Controls` to open the main section, and use the `time` slider and `Play/Stop loop` button to control the animation. Drag your mouse over the display to also rotate the picture while at it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this doesn't look much like the tissue we're supposed to simulate, does it?  We have spent all this computer time to simulate all these neurites making up the cell, we deserve more attractive visuals!\n",
    "\n",
    "Let's move on then to display the membrane voltage all over the cell, in glorious, 24 bit, false color! (You'd need fluo to see it on the real thing anyway.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying whole-neuron activity\n",
    "\n",
    "To observe what happens all over the cell, we'll have to *record* all over, and *display* the whole cell.  \n",
    "And we'll achieve this through some cool add-ons to EDEN that: \n",
    "- explain how spatially detailed cells are being simulated as discrete elements of neurite,\n",
    "- and how exactly these parts and the whole neuron look like as polygonal solids.\n",
    "\n",
    "First, we'll retrieve for this cell type, how many *compartments* is is cut into.  Each *compartment* in a model neuron is equivalent to a *pixel* in an image; it is a (hopefully) small bit of the neuron where we assume all electrical, chemical etc. properties are uniform throughout.\n",
    "\n",
    "We can get this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells_info = eden_simulator.experimental.explain_cell(nml_dir+\"/Sim.xml\")\n",
    "print(type(cells_info))\n",
    "print(cells_info.keys())\n",
    "cell_info = cells_info[celltype_name]\n",
    "print(cell_info.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each *physically modelled*\\* cell type, we got a set of lists, most with as many elements as there are compartments for each cell type.  We can then use the `comp_midpoint_segment` and `comp_midpoint_fractionAlong` lists to locate the middle of each compartment, and tap into that to record the membrane voltage for each part of each neuron.\n",
    "\n",
    "Given this information, let's record for each cell.  We'll also cut down on the sampling rate (using the Eden-specific extension `<EdenOutputFile>`) because that's way more than the one trajectory per cell we were recording before.\n",
    "\n",
    "To learn more about `explain_cell`, refer to its [Python API page]( python_api.rst#module-eden_simulator.experimental ).\n",
    "\n",
    "\\* that is, excluding artificial cells which typically have unique properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_mid_seg = cell_info['comp_midpoint_segment']\n",
    "comp_mid_fra = cell_info['comp_midpoint_fractionAlong']\n",
    "n_comps = len(comp_mid_seg) # Number of actual compartments in this cell, may be retrieved from most cell_info arrays\n",
    "n_comps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll construct a `SimMore.xml` file that records the voltage of *every single compartment* on *every single cell*.  \n",
    "\n",
    "We'll also cut down how often we record these waveforms to 0.5 msec (default NeuroML behaviour is to record for every single timestep which is quite a lot of steps, and we don't always need this much resolution.)\n",
    "\n",
    "Note the use of `<EdenOutputFile>`, an EDEN-specific version of the regular `<OutputFile>` with more recording options.  Another one of these is controlling the units per recorded trajectory in `output_units`, which we'll use to record membrane voltage in the more commonly used millivolts, this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a new routine for making the <Simulation> file with new recording options\n",
    "def NmlSimParmss(run_time, run_timestep=25e-6, sampling_period=1e-3,\n",
    "            rec_lines='', href='file://results.gen.txt'):\n",
    "    return f'''\n",
    "        <Simulation id=\"sim1\" length=\"{run_time}s\" step=\"{run_timestep}s\" target=\"Net\">\n",
    "            <EdenOutputFile id=\"first\" href=\"{href}\" format=\"ascii_v0\" sampling_interval=\"{sampling_period} s\">\n",
    "            '''+'\\n\\t        '.join(rec_lines)+'''\n",
    "            </EdenOutputFile>\n",
    "        </Simulation>\n",
    "        <Target component=\"sim1\"/>'''\n",
    "\n",
    "# Make the traces to record each compartment, and save the file\n",
    "traces = [f\"{population_name}[{neu}]/{celltype_name}/{comp_mid_seg[i]}{('%.9f'%comp_mid_fra[i])[1:]}/v\"\n",
    "    for neu in range(N_cells) for i in range(len(comp_mid_seg))]\n",
    "rec_lines = [f'<OutputColumn id=\"v_{i}\" quantity=\"{x}\"  output_units=\"mV\"/>' for i,x in enumerate(traces) ]\n",
    "print(f\"recording {len(rec_lines)} waveforms this time !\")\n",
    "with open(nml_dir+\"/SimMore.xml\", \"w\") as f:   \n",
    "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Lems>\\n<include file=\"example.nml\"/>\\n')\n",
    "    f.write(NmlSimParmss(run_time=0.25, run_timestep=25e-6, sampling_period=0.5e-3, href='./moresults.gen.txt', rec_lines=rec_lines))\n",
    "    f.write('\\n</Lems>\\n')\n",
    "# !cat $nml_dir/SimMore.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time moresults = eden_simulator.runEden(nml_dir+\"/SimMore.xml\",verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Running simulations with lots of output may often take appreciably more time just to write down and read back all these data ...  \n",
    "... now imagine storing all that, times a whole parameter space D:  \n",
    "(Though this time, we cut down the sampling rate by a lot so it balances out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have many more traces in the sim output, one per neuron per compartment!  \n",
    "\n",
    "Just in case we want differently-shaped neurons in the future, let's leave the traces list to be:\n",
    "- instead of making an array of `N_cells x n_comps x timesteps`, assign an index number in the sequence of traces, from which the set of traces start (n_comps trajectories, including the indexed one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "neuron_waveforms = np.array([moresults[x] for i,x in enumerate(traces)])\n",
    "print(neuron_waveforms.shape)\n",
    "# XXX append the offsets\n",
    "timevec = moresults['t']\n",
    "offset_per_neuron = np.round(np.arange(N_cells)*n_comps)\n",
    "print(offset_per_neuron)\n",
    "im = plt.imshow(neuron_waveforms,\n",
    "    extent=[ results['t'][0], timevec[-1], N_cells-.5,-0.5 ],\n",
    "    aspect='auto', interpolation='none', cmap =\"viridis\");cbar = plt.colorbar(im); cbar.set_label('Voltage (mV)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ran `explain_cell`, some lists `mesh_vertices`, `mesh_faces` and `mesh_comp_per_face` were also provided for the cell.  These represent the solid shape (in computer graphics parlance, a *mesh*) that cells of this type have. (For unique morphology, one would make individual cell types in NeuroML.)\n",
    "\n",
    "*Meshes* are made up from a set of points (known as *vertices*) in 3-D space, and a set of polygonal *faces* (typically triangles).  Thus the joined flat *faces* form the shape together, just like lines connected through lines do in 2D space.  To display a 3-D object, meshes may be enhanced with texture images and related attributes, to show a more detailed surface on the meshes.  But for our purpose, we'll be painting the mesh explicitly, so that we can show biophysical attributes across each cell.\n",
    "\n",
    "Here is what the neuron looks like according to the provided mesh, in plain color:  \n",
    "(Use the mouse over the picture to rotate/move the neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "viz = trimesh.Trimesh(\n",
    "    vertices=cell_info['mesh_vertices'],\n",
    "    faces=cell_info['mesh_faces'],\n",
    ")\n",
    "viz.visual.face_colors = (0.1,0.9,0.1)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that that if $z$ is up, that's a different orientation compared to the [render](https://neuroml-db.org/api/gif?id=NMLCL000625) in the NeuroML-DB.\n",
    "\n",
    "What the $\\overrightarrow x, ~ \\overrightarrow y,~ \\overrightarrow z ~$ directions represent is, unfortunately, up to the person who traced the neuron; there is no commonly agreed convention.  Hence, you may have to tweak the coordinates to your favourite coordinate system:  \n",
    "- Observe a discernible feature e.g. the axon;\n",
    "- then see which direction it extends to in the modeller's coordinates;\n",
    "- see which direction it should be following in your coordinates, and you'll understand how to map this axis.\n",
    "- If it's important, repeat for the other two axes (eg with planarity, or asymmetrical shape) to find out how to transform the other axes.\n",
    "\n",
    "For the following, we'll assume $\\,\\overrightarrow y~$ points to up and rotate the `plot.camera` accordingly, just like we did in the previous point-based animation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the `mesh`'s `vertices` and `faces` we got a list `mesh_comp_per_face`, which indicates which *compartment* is represented by each *face* of the mesh.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_vertices = cell_info['mesh_vertices']\n",
    "mesh_faces    = cell_info['mesh_faces'   ]\n",
    "face_comp = cell_info['mesh_comp_per_face']\n",
    "print(f\"We have {len(cell_info['mesh_faces'])} faces in the mesh, and {len(cell_info['mesh_comp_per_face'])} matching labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining this with the vertex numbers that each face is made of, we can have both a mapping from each compartment to each corresponding face, as well as from each compartment to each vertex. (Different 3D graphics utilities prefer each form.)\n",
    "\n",
    "We'll now use this mapping to paint the neuron selectively, with a different colour for each compartment, to show how membrand potential spreads when it's initiated from the soma.  We'll form the mapping as sparse matrices, to use the speed and clear coding of numpy over python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "# face_comp is a faces-sized vector with the compartment that each face represents.\n",
    "# mesh_faces is a faces x 3 array with the three vertices forming each face.\n",
    "# Assemble sparse matrices matching each compartment to irs corresponding faces, and vertices.\n",
    "# Use the coordinate format (v,(i,j)), to fill each nonzero (i[k],j[k]) with value v[k]\n",
    "n_comps = len(comp_mid_seg); n_faces = len(face_comp)\n",
    "# First for mapping each comp in sequence, to each in face_comp in sequence. (a binary matrix)\n",
    "faces_per_comp = scipy.sparse.coo_matrix( ( np.ones(n_faces) , (face_comp , range(n_faces)) ) , shape=(n_comps,n_faces) ).tocsr()\n",
    "# Now for the more involved part, getting all the vertices per compartment.\n",
    "iii = np.repeat(range(len(face_comp)),3) # for each of the compartments, for each vertex of the triangle\n",
    "jjj = mesh_faces.flatten() # for each face, for each vertex of the triangle\n",
    "vvv = np.ones(len(jjj)) # there is one vertex (or more LATER if vertices are being shared)\n",
    "verts_per_face = scipy.sparse.coo_matrix((vvv,(iii,jjj)),shape=(len(face_comp),len(mesh_vertices))).tocsr()\n",
    "verts_per_face /= np.ones(len(face_comp)) @ verts_per_face # for each vertex, divide by faces touching the same vertex. could also use bool ops LATER?\n",
    "verts_per_comp = faces_per_comp @ verts_per_face # TODO what about comps touching the same vertices ? how to handle those...\n",
    "# TODO use usual-handed matmult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with `faces_per_comp` and `verts_per_comp`, create a mesh object for each neuron, with one voltage value per vertex, and animated like the single points were previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from si_prefix import si_format\n",
    "print(f'Total compartment samples to display: {N_cells} cells * {n_comps} comps/cell * {len(timevec)} \\\n",
    "points in time = { si_format(N_cells * n_comps * len(timevec)) }')\n",
    "print(f'Total vertices * frames: {N_cells} cells * {len(mesh_vertices)} verts/cell * {len(timevec)} \\\n",
    "points in time = {  si_format(N_cells * len(mesh_vertices) * len(timevec)) }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This animation will involve a LOT of data, in fact too much data to easily publish.\n",
    "Hence the following figure is shown for one point in time.\n",
    "\n",
    "But you can get the full animation this way:\n",
    "- Run the notebook (on your computer or a <a href=\"#top\">web service</a>), and uncomment the indicated line 👇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO show the low-res version of neurons as well, it might be manageable.\n",
    "# Now make the animation data\n",
    "data_timestep = np.diff(timevec)[0] # assuming fixed timestep\n",
    "anim_fps = 60; real_sec_per_frame = 1/anim_fps # for animated display\n",
    "sim_sec_per_frame = 0.5e-3 # for sampling the data\n",
    "traj_samples_per_frame = round(sim_sec_per_frame/data_timestep)\n",
    "if np.any(traj_samples_per_frame == 0): raise ValueError('Some frames contain no samples, reduce sim_sec_per_frame or increase the data_timestep.')\n",
    "sample_for_frame = [int(0.135/data_timestep)] # get a small taste\n",
    "# 👇 uncomment the following for the full animation ! 👇\n",
    "sample_for_frame = range(0,len(timevec),traj_samples_per_frame)\n",
    "\n",
    "print(\"Animation duration: %d frames, %.3f sec\"\n",
    "      % (len(sample_for_frame), len(sample_for_frame)*real_sec_per_frame))\n",
    "import k3d; from k3d.colormaps import matplotlib_color_maps as k3dmaps\n",
    "plot = k3d.plot(camera_auto_fit=False) # to override camera orientation\n",
    "for neu in range(N_cells):\n",
    "    tra_off = offset_per_neuron[neu] # there some traces per neuron, starting from ...\n",
    "    comp_trajes = neuron_waveforms[tra_off:tra_off+n_comps,:] # there they are\n",
    "    k3d_anim_dict = {}; k3d_label_dict = {} # real time in sec string, to values\n",
    "    for frame,sample in enumerate(sample_for_frame):\n",
    "        real_time = frame * real_sec_per_frame\n",
    "        sim_time  = timevec[sample]\n",
    "        # for each vertex; units are already set to mVolt in EdenOutputFile !\n",
    "        k3d_anim_dict[str(real_time)] = (verts_per_comp.T @ comp_trajes[:,sample])\n",
    "        # k3d_anim_dict[str(real_time)] = (faces_per_comp.T @ comp_trajes[:,sample]) # color by vertex\n",
    "        k3d_label_dict[str(real_time)] = f't = {sim_time:.3f} s'\n",
    "    plt_mesh = k3d.mesh(mesh_vertices+cell_positions[neu,:], mesh_faces,#.astype('float32')astype('uint32')k3d_anim_dict\n",
    "        attribute=k3d_anim_dict, color_range=[-80, 0], color_map=k3dmaps.Rainbow); plot += plt_mesh\n",
    "        # triangles_attribute=k3d_anim_dict, color_range=[-80, 0], color_map=k3dmaps.Rainbow); plot += plt_mesh # color by triangle\n",
    "plot.camera = plot.get_auto_camera(pitch=30, yaw=10)[:6]+[0,1,0] # set 'y' to up !  TODO zoom a bit also; vecs are pos, tgt, up\n",
    "plt_label = k3d.text2d(k3d_label_dict, (0.,0.)); plot += plt_label # add 2d elements AFTER setting auto camera\n",
    "plot.fps = anim_fps;\n",
    "if len(sample_for_frame) > 1: plot.display(); plot.start_auto_play()\n",
    "else: plot.snapshot_type = 'inline'; display(HTML(plot.get_snapshot()))\n",
    "Multicomp_plot = plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Although the animation runs over several seconds, neurons fire quite fast; Use the `Play/Stop loop` button and the 'time' slider in `Controls`, to see -->\n",
    "Observe how the spikes travel along the neurites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "Now as the last thing, we'll fetch a screenshot to use in the documentation's [example gallery]( https://eden-simulator.org/gallery.html ).  \n",
    "We'll do this here by re-doing the whole plot, because it's too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Selenized browsers crash with BUFFER_SHORTAGE when the whole plot is loaded in one go :(\n",
    "# Thus: Re-make the animation data and render frame by frame\n",
    "lotp = k3d.plot(grid_visible=False,screenshot_scale=1,axes_helper=0,camera_auto_fit=False) # to override camera orientation\n",
    "lotp.colorbar_object_id = 0 # Disabling colorLegend programmatically\n",
    "\n",
    "data_timestep = np.diff(timevec)[0] # assuming fixed timestep\n",
    "sample_for_frame = range(int(0.1/data_timestep),int(0.103/data_timestep),traj_samples_per_frame)\n",
    "neuron_meshes = [None] * N_cells\n",
    "frames = []\n",
    "\n",
    "import time\n",
    "try:\n",
    "    from k3d.headless import k3d_remote, get_headless_driver\n",
    "    headless = k3d_remote(lotp, get_headless_driver(), width=500, height=500)\n",
    "    for neu in range(N_cells):\n",
    "        plt_mesh = k3d.mesh(mesh_vertices+cell_positions[neu,:], mesh_faces,#.astype('float32')astype('uint32')k3d_anim_dict\n",
    "            attribute=[99], color_range=[-80, 0], color_map=k3dmaps.Rainbow); lotp += plt_mesh\n",
    "        neuron_meshes[neu] = plt_mesh\n",
    "    lotp.camera = lotp.get_auto_camera(pitch=30, yaw=10)[:6]+[0,1,0] # set 'y' to up !  TODO zoom a bit also; vecs are pos, tgt, up\n",
    "    # lotp.camera = [-45.96106410442951, 178.83172993394766, 242.69050004118378, 163.18205388309028, -58.73859569108522, -262.78073580267176, 0,1,0]\n",
    "    headless.sync(hold_until_refreshed=True)\n",
    "    headless.camera_reset(.6)\n",
    "    for frame,sample in enumerate(sample_for_frame):\n",
    "        real_time = real_time = frame * real_sec_per_frame# real_times[frame]\n",
    "        for neu in range(N_cells):\n",
    "            tra_off = offset_per_neuron[neu] # there some traces per neuron, starting from ...\n",
    "            comp_trajes = neuron_waveforms[tra_off:tra_off+n_comps,:] # there they are\n",
    "            real_time = frame * real_sec_per_frame\n",
    "            sim_time  = timevec[sample]#frame * sim_sec_per_frame\n",
    "            # for each vertex; units are already set to mVolt in EdenOutputFile !\n",
    "            neuron_meshes[neu].attribute = (verts_per_comp.T @ comp_trajes[:,sample])\n",
    "        # lotp.time = real_time\n",
    "        lotp.camera[0]+=10\n",
    "        headless.sync()\n",
    "        time_start = time.time()\n",
    "        screenhot = headless.get_screenshot()\n",
    "        time_end = time.time()\n",
    "        # print(f\"{time_end - time_start} sec\")\n",
    "        frames.append(screenhot)\n",
    "    lotp.close()\n",
    "finally:\n",
    "    headless.close()\n",
    "import IPython\n",
    "IPython.display.Image(data=frames[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "\n",
    "# Stage 1: Crop\n",
    "frame_data = [ Image.open(io.BytesIO(x)) for x in frames]\n",
    "frame_data = [ x.crop((int(0.25*x.size[0]), 0, x.size[0], x.size[1])) for x in frame_data ]\n",
    "# frame_data = [ x.crop((int(0.25*x.size[0]), int(0.25*x.size[1]), x.size[0], x.size[1])) for x in frame_data ]\n",
    "\n",
    "# Stage 2: Resize\n",
    "for x in frame_data: x.thumbnail((240,200))\n",
    "    \n",
    "# Stage 3: Lossy - or not\n",
    "# frame_data = [x.convert('P',palette=Image.Palette.ADAPTIVE,dither=Image.Dither.NONE,colors=256) for x in frame_data]\n",
    "# to apng, or gif\n",
    "outbuf = io.BytesIO()\n",
    "frame_data[0].save(outbuf, format='gif', save_all=True, append_images=frame_data[1:], duration=100, loop=0)\n",
    "print('GIF size:',len(outbuf.getvalue()))\n",
    "\n",
    "# to alpha channel out - how is this not compressed already\n",
    "im = Image.open(outbuf)\n",
    "# https://github.com/python-pillow/Pillow/issues/3292#issuecomment-410837926\n",
    "newframes = [x.copy().convert('RGB') for x in PIL.ImageSequence.Iterator(im)]\n",
    "\n",
    "aabuf = io.BytesIO()\n",
    "newframes[0].save(aabuf, format='png', save_all=True, append_images=newframes[1:], duration=100, loop=0)\n",
    "print(len(aabuf.getvalue()))\n",
    "\n",
    "# to lossless recompression\n",
    "import oxipng\n",
    "oxipng_opts = {'level':6}\n",
    "aa = oxipng.optimize_from_memory(aabuf.getvalue(), **oxipng_opts)\n",
    "print('Optimized PNG size:',len(aa))\n",
    "\n",
    "# display(IPython.display.Image(data=outbuf.getvalue()))\n",
    "# display(IPython.display.Image(data=aabuf.getvalue()))\n",
    "# display(IPython.display.Image(data=aa))\n",
    "\n",
    "# IPython refuses to nbconvert gif files, and apng is not as efficient, we'll have to get creative ...\n",
    "# Accept having a png file for now? nah it's twice as big as the gif after gif compression.\n",
    "# and save the file under an explicit name bc nbsphinx is misbehaving\n",
    "# with open('_static/thumb_tut_net.png','wb') as f: f.write(aa)\n",
    "with open('_static/thumb_tut_net.gif','wb') as f: f.write(outbuf.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "And minimize plots for publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Ipywidgets plots are nice and all but they are widgets;\n",
    "# hence if they are just instantiated, whether displayed or not, they are being included in the exported ipynb or html. \n",
    "# Their contents are being saved in a much less efficient way than if the snapshot was made and displayed.\n",
    "# Because we have to instantiate them, we can't get rid of them\n",
    "# (I tried but something keeps tracking and adding them at the ipykernel comm level)\n",
    "# Hence: Minimize a plot's content to the least workable unit. \n",
    "# Applicable ONLY when the plot is not actually displayed (eg its snapshot is shown instead)\n",
    "def MinimizeK3dPlot(plot, more_objects=[]):\n",
    "    # If sth was removed, put it in more_objects for minimization\n",
    "    import k3d.objects as obs\n",
    "    import numpy as np\n",
    "    from traitlets import TraitError\n",
    "    type_to_attrs = {\n",
    "        'Mesh':['color_map','opacity_function','vertices','indices','normals','uvs','texture','attribute','triangles_attribute','volume','colors'],\n",
    "        'Points':['color_map','opacity_function','positions','colors','point_sizes','attribute'],\n",
    "        'Text2d':['text','position','size']\n",
    "    }\n",
    "    for ob in plot.objects+more_objects:\n",
    "        typ = ob.type\n",
    "        if typ not in type_to_attrs:\n",
    "            print(f'Type not supported: {typ}');break\n",
    "        for attrname in type_to_attrs[typ]:\n",
    "            attr = getattr(ob, attrname)\n",
    "            if not isinstance(attr, np.ndarray) and not attr : continue\n",
    "            if isinstance(attr,dict): fakev = {k:[] for k,v in attr.items()}\n",
    "            else: fakev = []\n",
    "            # print(attrname, fakev)\n",
    "            altvals = [0, None]\n",
    "            for v in [fakev] + altvals:\n",
    "                try: \n",
    "                    setattr(ob, attrname,v)\n",
    "                    if v is dict: setattr(ob, attrname,[])\n",
    "                except TraitError: continue\n",
    "                break\n",
    "            \n",
    "for x in [Point_plot, Multicomp_plot]: MinimizeK3dPlot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "!rm -rf tut_net/ # clean up TODO multiplatform XXX clean all files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
