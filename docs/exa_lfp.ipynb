{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Local field potential\n",
    "\n",
    "In this example, we show how one can sample the Local Field Potential around a neuron, by extracting the necessary data from the simulation and applying the LFP caclulations for the desired sample points.\n",
    "\n",
    "If one adds a population of neurons, does the relevant changes and focuses the sample points closer, the same methods can also yield the multi-unit activity (mind the simulation timestep though)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "First of all, let's install the required software, if on certain platorms like Colab that run the bare notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; from pathlib import Path\n",
    "if 'COLAB_BACKEND_VERSION' in os.environ:\n",
    "  !TMP=$(mktemp -d); git clone https://eden-simulator.org/repo --depth 1 -b development \"$TMP\"; cp -r \"$TMP/.\" .; rm -rf \"$TMP\"\n",
    "  exec(Path('.binder/install_livenb.py').read_text())\n",
    "if 'DEEPNOTE_PROJECT_ID' in os.environ: exec(Path('../.binder/install_livenb.py').read_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the model\n",
    "Let's see what the LFP looks like for the L5 pyramidal cell from [Mainen et al. 1995]( https://github.com/OpenSourceBrain/MainenEtAl_PyramidalCell ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the zip file with the model\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('https://codeload.github.com/OpenSourceBrain/MainenEtAl_PyramidalCell/zip/refs/heads/master', 'Mainen95.zip')\n",
    "# and unpack it\n",
    "from zipfile import ZipFile\n",
    "with ZipFile('Mainen95.zip', 'r') as zipp: zipp.extractall()\n",
    "# Check which files are present now\n",
    "# from os import listdir; listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the folder (through the GUI or os.listdir) and locate the NeuroMLv2 folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nml_dir = 'MainenEtAl_PyramidalCell-master/neuroConstruct/generatedNeuroML2/'\n",
    "# listdir(nml_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and run the simulation\n",
    "\n",
    "We'll run the existing setup with a lone neuron and a pulse clamp on it, but we'll make a whole new simulation file to capture all points of the neuron.\n",
    "\n",
    "First analyse the cell to extract the compartments this neurons contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_filename = nml_dir+\"MainenEtAl_PyramidalCell.net.nml\"\n",
    "celltype_name = 'MainenNeuroML'\n",
    "import eden_simulator\n",
    "cells_info = eden_simulator.experimental.explain_cell(net_filename)\n",
    "cell_info = cells_info[celltype_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_mid_seg = cell_info['comp_midpoint_segment']\n",
    "comp_mid_fra = cell_info['comp_midpoint_fractionAlong']\n",
    "n_comps = len(comp_mid_seg) # Number of actual compartments in this cell, may be retrieved from most cell_info arrays\n",
    "print(f'Compartments: {n_comps}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then generate the new simulation file with its traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_name = 'network_MainenEtAl_PyramidalCell';\n",
    "population_name = 'NeuroMLBased'; N_cells = 1\n",
    "# Set a new routine for making the <Simulation> file with new recording options\n",
    "def NmlSimParmss(network_name, run_time, run_timestep=25e-6, sampling_period=1e-3,\n",
    "            rec_lines='', href='file://results.gen.txt'):\n",
    "    return f'''\n",
    "        <Simulation id=\"sim1\" length=\"{run_time}s\" step=\"{run_timestep}s\" target=\"{network_name}\">\n",
    "            <EdenOutputFile id=\"first\" href=\"{href}\" format=\"ascii_v0\" sampling_interval=\"{sampling_period} s\">\n",
    "            '''+'\\n\\t        '.join(rec_lines)+'''\n",
    "            </EdenOutputFile>\n",
    "        </Simulation>\n",
    "        <Target component=\"sim1\"/>'''\n",
    "\n",
    "# Make the traces to record each compartment, and save the file\n",
    "traces = [f\"{population_name}[{neu}]/{celltype_name}/{comp_mid_seg[i]}{('%.9f'%comp_mid_fra[i])[1:]}/v\"\n",
    "    for neu in range(N_cells) for i in range(len(comp_mid_seg))]\n",
    "rec_lines = [f'<OutputColumn id=\"v_{i}\" quantity=\"{x}\"  output_units=\"mV\"/>' for i,x in enumerate(traces) ]\n",
    "print(f\"recording {len(rec_lines)} waveforms this time !\")\n",
    "sim_filename = nml_dir+\"/SimLFP.xml\"\n",
    "with open(sim_filename, \"w\") as f:   \n",
    "    f.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Lems>\\n<include file=\"MainenEtAl_PyramidalCell.net.nml\"/>\\n')\n",
    "    f.write(NmlSimParmss(network_name, run_time=0.06, run_timestep=10e-6, sampling_period=0.1e-3, href='moresults.gen.txt', rec_lines=rec_lines))\n",
    "    f.write('\\n</Lems>\\n')\n",
    "# !cat $nml_dir/SimMore.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time moresults = eden_simulator.runEden(sim_filename)\n",
    "timevec = moresults['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pint.registry import Quantity as Q\n",
    "neuron_membrane_voltage = np.array([moresults[x] for i,x in enumerate(traces)])\n",
    "neuron_membrane_voltage = Q(neuron_membrane_voltage,'mV')\n",
    "print(\"%d points in space, sampled over %d points in time.\" % neuron_membrane_voltage.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a raster of $V_m$ per compartment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "im = plt.imshow(neuron_membrane_voltage.m_as('mV'),\n",
    "    extent=[ timevec[0], timevec[-1], n_comps, 0 ],\n",
    "    aspect='auto', interpolation='none', cmap =\"viridis\")\n",
    "cbar = plt.colorbar(im); cbar.set_label('Voltage (mV)')\n",
    "plt.clim((-50, +50)); plt.ylabel('Compartment #'); plt.xlabel('Time (sec)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And an animation of said $V_m$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the 3D mesh and compartment-to triangle mapping, this will be included in the tooling soon: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "mesh_comp_per_face = cell_info['mesh_comp_per_face']\n",
    "mesh_vertices = cell_info['mesh_vertices']\n",
    "mesh_faces = cell_info['mesh_faces']\n",
    "mesh_faces = np.array(mesh_faces)\n",
    "n_comps = len(cell_info['comp_parent']); n_faces = len(mesh_faces)\n",
    "# face_comp is a faces-sized vector with the compartment that each face represents.\n",
    "# mesh_faces is a faces x 3 array with the three vertices forming each face.\n",
    "# Assemble sparse matrices matching each compartment to irs corresponding faces, and vertices.\n",
    "# Use the coordinate format (v,(i,j)), to fill each nonzero (i[k],j[k]) with value v[k]\n",
    "# First for mapping each comp in sequence, to each in face_comp in sequence. (a binary matrix)\n",
    "faces_per_comp = scipy.sparse.coo_matrix( ( np.ones(n_faces) , (mesh_comp_per_face , range(n_faces)) ) , shape=(n_comps,n_faces) ).tocsr()\n",
    "# Now for the more involved part, getting all the vertices per compartment.\n",
    "iii = np.repeat(range(n_faces),3) # for each of the compartments, for each vertex of the triangle\n",
    "jjj = mesh_faces.flatten() # for each face, for each vertex of the triangle\n",
    "vvv = np.ones(len(jjj)) # there is one vertex (or more LATER if vertices are being shared)\n",
    "verts_per_face = scipy.sparse.coo_matrix((vvv,(iii,jjj)),shape=(len(mesh_comp_per_face),len(mesh_vertices))).tocsr()\n",
    "verts_per_face /= np.ones(n_faces) @ verts_per_face # for each vertex, divide by faces touching the same vertex. could also use bool ops LATER?\n",
    "verts_per_comp = faces_per_comp @ verts_per_face\n",
    "def mapped_verts(comp_cols): return verts_per_comp.T @ comp_cols\n",
    "def mapped_faces(comp_cols): return faces_per_comp.T @ comp_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k3d; from k3d.colormaps import matplotlib_color_maps as k3dmaps\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "data_timestep = np.diff(timevec)[0] # assuming fixed timestep\n",
    "anim_fps = 24; real_sec_per_frame = 1/anim_fps # for animated displayfloa\n",
    "sim_sec_per_frame = 0.25e-3 # for sampling the data\n",
    "traj_samples_per_frame = round(sim_sec_per_frame/data_timestep)\n",
    "if np.any(traj_samples_per_frame == 0): raise ValueError('Some frames contain no samples, reduce sim_sec_per_frame or increase the data_timestep.')\n",
    "neuron_waveforms = neuron_membrane_voltage.m_as('mV')\n",
    "sample_for_frame = range(0,min(len(timevec),neuron_waveforms.shape[-1]),traj_samples_per_frame)\n",
    "print(\"Animation duration: %d frames, %.3f sec\"\n",
    "      % (len(sample_for_frame), len(sample_for_frame)*real_sec_per_frame))\n",
    "\n",
    "plot = k3d.plot(camera_auto_fit=False) # to override camera orientation\n",
    "k3d_anim_dict = {}; k3d_lfp_dict = {}; k3d_label_dict = {} # real time in sec string, to values\n",
    "for frame,sample in enumerate(sample_for_frame):\n",
    "    real_time = frame * real_sec_per_frame\n",
    "    sim_time  = timevec[sample]\n",
    "    # for each vertex; units are already set to mVolt in EdenOutputFile !\n",
    "    k3d_anim_dict[str(real_time)] = (verts_per_comp.T @ neuron_waveforms[:,sample]).astype(np.float32)\n",
    "   \n",
    "    k3d_label_dict[str(real_time)] = f't = {sim_time:.3f} s'\n",
    "plt_mesh = k3d.mesh(mesh_vertices.astype(np.float32), mesh_faces.astype(np.uint32),\n",
    "    attribute=k3d_anim_dict, color_range=[-70, -20], color_map=k3dmaps.Rainbow); plot += plt_mesh\n",
    "plot.camera = plot.get_auto_camera(pitch=30, yaw=10)[:6]+[0,1,0] # set 'y' to up ! vecs are pos, tgt, up\n",
    "plt_label = k3d.text2d(k3d_label_dict, (0.,0.)); plot += plt_label # add 2d elements AFTER setting auto camera\n",
    "plot.fps = anim_fps;\n",
    "if False: plot.display(); plot.start_auto_play()\n",
    "else: plot.snapshot_type = 'inline'; display(HTML(plot.get_snapshot()))\n",
    "Vm_plot = plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-process simulation data to get $I_m$\n",
    "\n",
    "We have recorded $V_m$ all over the cell and we'll use it (along with the cell's properties) to obtain the resulting LFP.\n",
    "\n",
    "The usual assumptions apply for this example: The extracellular medium is uniform, highly conductive, isotropic and unobstructed until far away.  If you need a more sophisticated model, apply the relevant adjustments to the following.  (Perhaps a separate simulation process just for the LFP is called for, see the other articles TBD on \"Pipelines\" generally.)\n",
    "\n",
    "If storing all voltage traces is becoming a burden, one can calculate the coupling matrix as they prefer and add up the numbers using virtual \"gap junctions\".  (The virtual gap junction method can be shown in a TBA article).  \n",
    "For really large numbers of current sources, it might become more efficient to stream values with a pipeline to the Barnes-Hut algorithm or the fast multipole method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll be looking at the LFP, hence we need $I_m$ .  Instead of adding up the current from each mechanism, we can estimate it through $dV_m/dt$. To reduce sampling artifacts, we could convolve with a [derivative of Gaussian]( https://inst.eecs.berkeley.edu/~cs194-26/fa17/Lectures/ConvEdgesTemplate.pdf ) in the place of `np.diff` but it's not a concern here.\n",
    "\n",
    "The dynamics of membrane voltage per compartment, are as follows: \n",
    "$$C\\dot{V_m} = I_{axial} + I_m + I_{injected}$$ \n",
    "Therefore we should subtract the current from adjacent compartments and the current from direct injection through the membrane, to isolate the trans-membrane current.\n",
    "\n",
    "*Note:* Although direct injections cross the membrane (topologically), they don't count towards the LFP because charge travels from inside a body to inside another; thus the extracellular environment is not affected.  \n",
    "Examples of direct injections are current clamps, as well as gap junctions between cells.\n",
    "\n",
    "Whether a stimulus from our rig should count as trans-membrane current or direct injection is ultimately up to the modeller's intention; due caution is advised. (eg. is a \"virtual synapse\" stimulus imposed by a clamp, or a simulated synapse?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# J = dQ/dt = C * dV/dt\n",
    "rec_dt = Q(np.diff(timevec)[0],'sec')\n",
    "comp_capa = Q(cell_info['comp_capacitance'],'pF')\n",
    "\n",
    "neuron_comp_flux = comp_capa[:,None]*(np.diff(neuron_membrane_voltage)/rec_dt)\n",
    "neuron_comp_flux.ito('pA') # Convert to specific units this way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the total current flow for each compartment.\n",
    "\n",
    "The simulator provides absolute numbers for unevenly sized compartments, the *density* over the membrane is perhaps more informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(18,5))\n",
    "im = axs[0].imshow(neuron_comp_flux.m_as('pA'),\n",
    "    extent=[ moresults['t'][0], timevec[-1], n_comps, 0 ],\n",
    "    aspect='auto', interpolation='none', cmap =\"bwr_r\");\n",
    "cbar = plt.colorbar(im); cbar.set_label('pA')\n",
    "im.set_clim((-30, +30)); axs[0].set_ylabel('Compartment #'); axs[0].set_xlabel('Time (sec)');\n",
    "axs[0].set_title(\"Comp. current\")\n",
    "# Now, get the density\n",
    "comp_area = Q(cell_info['comp_area'], 'um*um')\n",
    "neuron_comp_flux_density = neuron_comp_flux/comp_area[:,None]\n",
    "im = axs[1].imshow(neuron_comp_flux_density.m_as('pA/um**2'),\n",
    "    extent=[ moresults['t'][0], timevec[-1], n_comps, 0 ],\n",
    "    aspect='auto', interpolation='none', cmap =\"bwr_r\");\n",
    "cbar = plt.colorbar(im); cbar.set_label('pA/Î¼mÂ²')\n",
    "im.set_clim((-.4, +.4)); axs[1].set_ylabel('Compartment #'); axs[1].set_xlabel('Time (sec)');\n",
    "axs[1].set_title(\"Current density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the total current flux in/out of the compartment, where `+` is for current `in` and `-` for current `out` of the cell (of course, the reverse applies for electrons and negative ions).  \n",
    "We use the `bwr_r` colormap where blue is for positive flow `in`ward, because this flow will cause an opposite, *negative* potential outside the cell - which we'll get to see in the calculated LFP.\n",
    "\n",
    "To correct the numbers, let's calculate and subtract the \"axial\" current flowing between compartments, based on the known electrical conductance between them.  \n",
    "In this example, axial current is far far less than true trans-membrane current so there's no obvious difference, we should do it nonetheless.\n",
    "\n",
    "### Correcting for axial current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get the axial resistance matrix: Ia = G*V. It follows the tree structure\n",
    "comp_ga = cell_info['comp_conductance_to_parent']\n",
    "iii = []; jjj = []; vvv = [] # Construct the lists (i,j,v)\n",
    "for i,p in enumerate(cell_info['comp_parent']):\n",
    "    g = comp_ga[i]\n",
    "    if p >= 0:\n",
    "        iii += [ i, p, i, p] # add 4 sparse elms in one go\n",
    "        jjj += [ p, i, i, p]\n",
    "        vvv += [+g,+g,-g,-g] # same as:\n",
    "        # axial_conductance_matrix[i,p] += g\n",
    "        # axial_conductance_matrix[p,i] += g\n",
    "        # axial_conductance_matrix[i,i] -= g\n",
    "        # axial_conductance_matrix[p,p] -= g\n",
    "\n",
    "import scipy\n",
    "G = scipy.sparse.coo_matrix((vvv,(iii,jjj))).tocsr() # needs csr for some reason'\n",
    "G = Q(G,'nS')\n",
    "\n",
    "# One would think Ia = G*V directly, but the discretisation of the neuron \n",
    "# leads to rough Î”V gradient between compartments in practice (ploÏ„ Vm of the stilized axon, last 125 comps, around sample 100 to see). \n",
    "# -> hence Ia would come out extremely rough and inaccurate.\n",
    "# In its place we will use Bwd Euler to estimate the currents (neglecting ion channel conductance tbh):\n",
    "# Ia = C*dV/dt = G*V => Ia*dt = Vnext-Vnow = dt*inv(C)*G*Vnext, Vnext = inv(I-dt*inv(C)*G)*Vnow.\n",
    "# Then, make the dimensionless \"dynamics matrix\" that will give us Vnext from vnow.\n",
    "\n",
    "I = scipy.sparse.eye(n_comps); invC = (scipy.sparse.diags(1/comp_capa.m))\n",
    "# neither pint nor quantities can do matmul :c\n",
    "dynamics_matrix = I - (\n",
    "        (rec_dt.units*G.units/comp_capa.units) * (rec_dt.m*invC @ G.m)\n",
    "    ).to('1').m # cast to proper quantity, not eg. meters/km\n",
    "\n",
    "\n",
    "def get_true_membrane_current(comp_flux, membrane_voltage, G, time=0): \n",
    "    # instead of direct: axial_current = (G.m @ membrane_voltage.m)*G.units*membrane_voltage.units\n",
    "    vNext_axial = Q(scipy.sparse.linalg.spsolve(dynamics_matrix, membrane_voltage.m_as('mV')),'mV')\n",
    "    axial_current = (vNext_axial -  membrane_voltage) * comp_capa[:,None] / rec_dt\n",
    "    return (comp_flux - axial_current)\n",
    "neuron_membrane_current = get_true_membrane_current(neuron_comp_flux, neuron_membrane_voltage[:,:-1], G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: because direct currents such as patch clamps come from insulated wires, they *don't* affect the observed LFP, hence it is well possible that a 'monopole' appears during such stimulation!\n",
    "\n",
    "Kirchhoff's law still stands: $$I_{membrane} = -(I_{capacitive} + I_{injected})$$\n",
    "it's just that we can't measure the right hand side through extracellular space.\n",
    "\n",
    "If you're interested, repeat the graph from above to plot the difference between `neuron_comp_flux` and `neuron_membrane_current`, in absolute and per area terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intentionally left blank "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that come compartments are much more affected by axial current than others.  \n",
    "Generally, the less area they have, and the tighter they are coupled to adjacent compartments, the more affected they are by axial current. (Cytoplasmic resistivity `Ra` and membrane spec. capacitance `Cm` can also matter when non-uniform.)  \n",
    "Feel free to plot `comp_ga` and `comp_capa` (and the ratio thereof `1/RC`) to see which these may be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "Let's see then how the trans-membrane current evolves on the neuron.  We'll use the same Red-White-Blue colormap we'll display the LFP with, in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k3d; from k3d.colormaps import matplotlib_color_maps as k3dmaps\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "data_timestep = np.diff(timevec)[0] # assuming fixed timestep\n",
    "anim_fps = 24; real_sec_per_frame = 1/anim_fps # for animated displayfloa\n",
    "sim_sec_per_frame = 0.25e-3 # for sampling the data\n",
    "traj_samples_per_frame = round(sim_sec_per_frame/data_timestep)\n",
    "if np.any(traj_samples_per_frame == 0): raise ValueError('Some frames contain no samples, reduce sim_sec_per_frame or increase the data_timestep.')\n",
    "sample_for_frame = range(0,min(len(timevec),neuron_membrane_current.shape[-1]),traj_samples_per_frame)\n",
    "print(\"Animation duration: %d frames, %.3f sec\"\n",
    "      % (len(sample_for_frame), len(sample_for_frame)*real_sec_per_frame))\n",
    "neuron_waveforms = neuron_membrane_voltage.m_as('mV')\n",
    "plot = k3d.plot(camera_auto_fit=False) # to override camera orientation\n",
    "k3d_anim_dict = {}; k3d_lfp_dict = {}; k3d_label_dict = {} # real time in sec string, to values\n",
    "for frame,sample in enumerate(sample_for_frame):\n",
    "    real_time = frame * real_sec_per_frame\n",
    "    sim_time  = timevec[sample]\n",
    "    # for each vertex; units are already set to mVolt in EdenOutputFile !\n",
    "    if sample < neuron_membrane_current.shape[-1]:\n",
    "        Im = neuron_membrane_current[:,sample]\n",
    "        # print((Im/cell_info['comp_area']).shape)\n",
    "        k3d_anim_dict[str(real_time)] = (verts_per_comp.T @ (Im/comp_area).m_as('pA/um**2')).astype(np.float32)\n",
    "   \n",
    "    k3d_label_dict[str(real_time)] = f't = {sim_time:.3f} s'\n",
    "    # break\n",
    "plt_mesh = k3d.mesh(mesh_vertices.astype(np.float32), mesh_faces.astype(np.uint32),\n",
    "    attribute=k3d_anim_dict, color_range=[-.2, +.2], color_map=k3dmaps.Bwr); plot += plt_mesh\n",
    "plot.camera = plot.get_auto_camera(pitch=30, yaw=10)[:6]+[0,1,0] # set 'y' to up ! vecs are pos, tgt, up\n",
    "plt_label = k3d.text2d(k3d_label_dict, (0.,0.)); plot += plt_label # add 2d elements AFTER setting auto camera\n",
    "plot.fps = anim_fps;\n",
    "if False: plot.display(); plot.start_auto_play()\n",
    "else: plot.snapshot_type = 'inline'; display(HTML(plot.get_snapshot()))\n",
    "Im_plot = plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-process $I_m$ to get the LFP\n",
    "\n",
    "The other half of calculating the LFP is how much each point outside the cell is being affected by the current flow on each part of the neuron (that is, mostly by the closest compartments, but also slightly from the more distant ones).\n",
    "\n",
    "Just for illustration, we'll use one element per compartment. It would be even better if each NeuroML `<segment>` is applied separately, if it's not too much for the computer.  \n",
    "Alternatively, the LFPykit could also be used to calculate the coupling matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracell_conductivity = Q(.3, 'S/m')\n",
    "\n",
    "source_points = cell_info['comp_midpoint'][:]\n",
    "x_space = np.linspace(-360, 000, 10); y_space = np.linspace(-180, 600, 30); z_space = [-30]\n",
    "x, y, z = np.meshgrid(x_space, y_space, z_space, indexing='ij')\n",
    "sampling_points = np.stack((x,y,z),axis=-1).reshape((-1,3))\n",
    "# print(\"Sampling points:\", sampling_points)\n",
    "\n",
    "# Vectorize calculation of pairwise distance matrix https://jaykmody.com/blog/distance-matrices-with-numpy/\n",
    "distmat =  source_points @ sampling_points.T#np.sum(,axis=-1)\n",
    "distmat = np.sum(sampling_points**2,axis=-1) - 2*distmat + np.sum(source_points**2,axis=-1)[:,None]\n",
    "distmat = np.sqrt(distmat)\n",
    "distmat = Q(distmat, 'um')\n",
    "\n",
    "# TODO: Exclude current from points inside the cell, thereby avoiding singularities as a bonus.\n",
    "coupling = (1e-0 / (4*np.pi*extracell_conductivity*distmat)).to('uV/pA')\n",
    "\n",
    "print('Coupling is calculated for %d current sources times %d sampling points.' % coupling.shape)\n",
    "def get_lfp(membrane_current): return ((coupling.m.T @ -membrane_current.m)*coupling.units*membrane_current.units).to('uV')\n",
    "# get_lfp(neuron_membrane_current[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's show the membrane voltage and the observed LFP together in one animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k3d; from k3d.colormaps import matplotlib_color_maps as k3dmaps\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "data_timestep = np.diff(timevec)[0] # assuming fixed timestep\n",
    "anim_fps = 24; real_sec_per_frame = 1/anim_fps # for animated displayfloa\n",
    "sim_sec_per_frame = 0.25e-3 # for sampling the data\n",
    "traj_samples_per_frame = round(sim_sec_per_frame/data_timestep)\n",
    "if np.any(traj_samples_per_frame == 0): raise ValueError('Some frames contain no samples, reduce sim_sec_per_frame or increase the data_timestep.')\n",
    "sample_for_frame = range(0,min(len(timevec),neuron_membrane_current.shape[-1]),traj_samples_per_frame)\n",
    "print(\"Animation duration: %d frames, %.3f sec\"\n",
    "      % (len(sample_for_frame), len(sample_for_frame)*real_sec_per_frame))\n",
    "neuron_waveforms = neuron_membrane_voltage.m_as('mV')\n",
    "plot = k3d.plot(camera_auto_fit=False) # to override camera orientation\n",
    "k3d_anim_dict = {}; k3d_lfp_dict = {}; k3d_label_dict = {} # real time in sec string, to values\n",
    "for frame,sample in enumerate(sample_for_frame):\n",
    "    real_time = frame * real_sec_per_frame\n",
    "    sim_time  = timevec[sample]\n",
    "    k3d_anim_dict[str(real_time)] = (verts_per_comp.T @ neuron_waveforms[:,sample]).astype(np.float32)# for each vertex\n",
    "    if sample < neuron_membrane_current.shape[-1]:\n",
    "        lfp_now = get_lfp(neuron_membrane_current[:,sample]).m_as('uV')\n",
    "        k3d_lfp_dict[str(real_time)] = \\\n",
    "            lfp_now.astype(np.float32)\n",
    "   \n",
    "    k3d_label_dict[str(real_time)] = f't = {sim_time:.3f} s'\n",
    "\n",
    "lfp_scale_range = [-2, 2] #other options to see transitions vs. absolute range: [-.2, .2], [-.5, +.5] ... \n",
    "plt_points = k3d.points(positions=sampling_points.astype(np.float32),attribute=k3d_lfp_dict,color_range=lfp_scale_range,point_size=10,color_map=k3dmaps.Bwr); plot += plt_points\n",
    "plot.camera = plot.get_auto_camera(pitch=30, yaw=10)[:6]+[0,1,0] # set 'y' to up ! vecs are pos, tgt, up\n",
    "plt_mesh = k3d.mesh(mesh_vertices.astype(np.float32), mesh_faces.astype(np.uint32),\n",
    "    attribute=k3d_anim_dict, color_range=[-70, -20], color_map=k3dmaps.Rainbow); plot += plt_mesh\n",
    "plt_label = k3d.text2d(k3d_label_dict, (0.,0.)); plot += plt_label # add 2d elements AFTER setting auto camera\n",
    "plot.fps = anim_fps;\n",
    "if False: plot.display(); plot.start_auto_play()\n",
    "else: plot.snapshot_type = 'inline'; display(HTML(plot.get_snapshot()))\n",
    "LFP_plot = plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dots around the neuron are the sampling points for the LFP, they don't necessariluy have to be a regular grid (but it's easier to understand this way).  \n",
    "They are coloured with red â¤ï¸ for positive potential, blue ðŸ’™ for negative, and white ðŸ¤ for zero.\n",
    "\n",
    "Observe that the LFP goes negative (blue) near where the cell depolarizes, and positive (red) where the cell repolarizes. (Also depnding on the activity of the more distant parts, of course.)\n",
    "                                                                                                                           \n",
    "As usual, use `Controls > Play/Stop loop` to control the animation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More ideas\n",
    "\n",
    "The enterprising reader may find more nuances and improved ways of calculating the LFP in the work of, among others, Einevoll and his lab. \n",
    "\n",
    "For best results, consider using non-uniform sampling grids, centered around where the potential changes most sharply and where the (both stimulation and recording) electrodes are located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "Now as the last thing, we'll fetch a screenshot to use in the documentation's [example gallery]( https://eden-simulator.org/gallery.html ).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "plot = LFP_plot\n",
    "plot.grid_visible = False\n",
    "plot.screenshot_scale = 1\n",
    "plot.axes_helper = 0\n",
    "plot.colorbar_object_id = 0 # Disabling colorLegend programmatically\n",
    "plt_points.color_range = [-1, +1]\n",
    "plt_points.point_size = 15\n",
    "plt_label.visible=False\n",
    "plot -= plt_label\n",
    "data_timestep = np.diff(timevec)[0] # assuming fixed timestep\n",
    "# sample_for_frame = \n",
    "print(len(list(k3d_anim_dict.keys())))\n",
    "framets = [y[x] for y in [list(k3d_anim_dict.keys())] for x in range(int(0.031/(data_timestep*traj_samples_per_frame)),int(0.031/(data_timestep*traj_samples_per_frame))+1,traj_samples_per_frame)]\n",
    "frames = []\n",
    "# for frame,sample in enumerate(sample_for_frame):print(sample_for_frame)\n",
    "# print(framets)\n",
    "\n",
    "# print(plt_mesh.attribute)\n",
    "# raise\n",
    "import time\n",
    "try:\n",
    "    from k3d.headless import k3d_remote, get_headless_driver\n",
    "    headless = k3d_remote(plot, get_headless_driver(), width=500, height=300)\n",
    "#     # plot.camera = lotp.get_auto_camera(pitch=30, yaw=10)[:6]+[0,1,0] # set 'y' to up !  TODO zoom a bit also; vecs are pos, tgt, up\n",
    "    plot.camera = [0, 0, +200, 0,0,0, -1,0,0]\n",
    "    headless.sync(hold_until_refreshed=True)\n",
    "    headless.camera_reset(.49)\n",
    "    # Set each attribute because does headless not like dicts ...\n",
    "    for frame,real_time in enumerate(framets):\n",
    "        # need to clear before reassigning...\n",
    "        for x in plt_mesh, plt_points: x.attribute = {k:[] for k,v in x.attribute.items()}\n",
    "        \n",
    "        plt_mesh.attribute = k3d_anim_dict[str(real_time)]\n",
    "        plt_points.attribute = k3d_lfp_dict[str(real_time)]\n",
    "\n",
    "        headless.sync()\n",
    "        time_start = time.time()\n",
    "        screenhot = headless.get_screenshot()\n",
    "        time_end = time.time()\n",
    "        # print(f\"{time_end - time_start} sec\")\n",
    "        frames.append(screenhot)\n",
    "finally:\n",
    "    headless.close()\n",
    "import IPython\n",
    "# IPython.display.Image(data=frames[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "\n",
    "# Stage 1: Crop\n",
    "frame_data = [ Image.open(io.BytesIO(x)) for x in frames]\n",
    "frame_data = [ x.crop((int(0.3*x.size[0]), int(0.2*x.size[1]), int((1-.01)*x.size[0]), int((1-0.1)*x.size[1]))) for x in frame_data ]\n",
    "# frame_data = [ x.crop((int(0.25*x.size[0]), int(0.25*x.size[1]), x.size[0], x.size[1])) for x in frame_data ]\n",
    "\n",
    "# Stage 2: Resize\n",
    "for x in frame_data: x.thumbnail((240,200))\n",
    "    \n",
    "# Stage 3: Lossy - or not\n",
    "# frame_data = [x.convert('P',palette=Image.Palette.ADAPTIVE,dither=Image.Dither.NONE,colors=256) for x in frame_data]\n",
    "# to apng, or gif\n",
    "outbuf = io.BytesIO()\n",
    "frame_data[0].save(outbuf, format='gif', save_all=True, append_images=frame_data[1:], duration=100, loop=0)\n",
    "print('GIF size:',len(outbuf.getvalue()))\n",
    "\n",
    "# to alpha channel out - how is this not compressed already\n",
    "im = Image.open(outbuf)\n",
    "# https://github.com/python-pillow/Pillow/issues/3292#issuecomment-410837926\n",
    "newframes = [x.copy().convert('RGB') for x in PIL.ImageSequence.Iterator(im)]\n",
    "\n",
    "aabuf = io.BytesIO()\n",
    "newframes[0].save(aabuf, format='png', save_all=True, append_images=newframes[1:], duration=100, loop=0)\n",
    "print(len(aabuf.getvalue()))\n",
    "\n",
    "# to lossless recompression\n",
    "import oxipng\n",
    "oxipng_opts = {'level':6}\n",
    "aa = oxipng.optimize_from_memory(aabuf.getvalue(), **oxipng_opts)\n",
    "print('Optimized PNG size:',len(aa))\n",
    "\n",
    "# display(IPython.display.Image(data=outbuf.getvalue()))\n",
    "# display(IPython.display.Image(data=aabuf.getvalue()))\n",
    "display(IPython.display.Image(data=aa))\n",
    "\n",
    "# IPython refuses to nbconvert gif files, and apng is not as efficient, we'll have to get creative ...\n",
    "# Accept having a png file for now? nah it's twice as big as the gif after gif compression.\n",
    "# and save the file under an explicit name bc nbsphinx is misbehaving\n",
    "with open('_static/thumb_exa_lfp.png','wb') as f: f.write(aa)\n",
    "# with open('_static/thumb_tut_net.gif','wb') as f: f.write(outbuf.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbsphinx": "hidden"
   },
   "source": [
    "And minimize plots for publishing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# Ipywidgets plots are nice and all but they are widgets;\n",
    "# hence if they are just instantiated, whether displayed or not, they are being included in the exported ipynb or html. \n",
    "# Their contents are being saved in a much less efficient way than if the snapshot was made and displayed.\n",
    "# Because we have to instantiate them, we can't get rid of them\n",
    "# (I tried but something keeps tracking and adding them at the ipykernel comm level)\n",
    "# Hence: Minimize a plot's content to the least workable unit. \n",
    "# Applicable ONLY when the plot is not actually displayed (eg its snapshot is shown instead)\n",
    "def MinimizeK3dPlot(plot, more_objects=[]):\n",
    "    # If sth was removed, put it in more_objects for minimization\n",
    "    import k3d.objects as obs\n",
    "    import numpy as np\n",
    "    from traitlets import TraitError\n",
    "    type_to_attrs = {\n",
    "        'Mesh':['color_map','opacity_function','vertices','indices','normals','uvs','texture','attribute','triangles_attribute','volume','colors'],\n",
    "        'Points':['color_map','opacity_function','positions','colors','point_sizes','attribute'],\n",
    "        'Text2d':['text','position','size']\n",
    "    }\n",
    "    for ob in plot.objects+more_objects:\n",
    "        typ = ob.type\n",
    "        if typ not in type_to_attrs:\n",
    "            print(f'Type not supported: {typ}');break\n",
    "        for attrname in type_to_attrs[typ]:\n",
    "            attr = getattr(ob, attrname)\n",
    "            if not isinstance(attr, np.ndarray) and not attr : continue\n",
    "            if isinstance(attr,dict): fakev = {k:[] for k,v in attr.items()}\n",
    "            else: fakev = []\n",
    "            # print(attrname, fakev)\n",
    "            altvals = [0, None]\n",
    "            for v in [fakev] + altvals:\n",
    "                try: \n",
    "                    setattr(ob, attrname,v)\n",
    "                    if v is dict: setattr(ob, attrname,[])\n",
    "                except TraitError: continue\n",
    "                break\n",
    "            \n",
    "for x in [Vm_plot, Im_plot, LFP_plot]: MinimizeK3dPlot(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
